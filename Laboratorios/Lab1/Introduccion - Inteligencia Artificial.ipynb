{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Básico de Aprendizaje Automático"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este laboratorio incluye las herramientas basicas y esenciales de aprendizaje automático.  Dará un vistazo de como o que hay debajo de la inteligencia artificial.\n",
    "\n",
    "Revisaremos algebra lineal y probabilidades, luego nos moveremos a la construcción de los fundamentos de aprendizaje automatico de algoritmos y sistemas.\n",
    "\n",
    "El laboratorio cubre:\n",
    "\n",
    "- Matemática Basica Aplicada\n",
    "- Teoría de Probabilidades\n",
    "- Construcción de algoritmos básicos de aprendizaje automatico\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matemática Básica Aplicada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matemática es la base de resolución de problemas, y esto también se da para intelignecia artificial.  Con librerías como numpy podemos eliminar lazos y hacer algebra lineal facil.  Cuando se piensa en IA, la mayoría de los miles de miles de operaciones que tienen que hacer el computador en tiempo real se dan basados a los bloques bases de algebra lineal para ayudarnos de manera programática."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "## Multiplicación de elementos con lazos\n",
    "x = [3,4,5]\n",
    "y = [0.2,0.4,0.6]\n",
    "product = []\n",
    "for i in range(len(x)):\n",
    "    product.append(x[i]*y[i])\n",
    "\n",
    "## Multiplicación de productos con algebra lineal\n",
    "x = np.array([3,4,5])\n",
    "y = np.array([0.2,0.4,0.6])\n",
    "x * y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Los Bloques Básicos de Inteligencia Artifical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las aplicaciones de aprendizaje profundo son construidas o se dan en base a bloques de algebra lineal; escalares, vecotres, matrices y tensores.  Cada linea inferior ilustra lo anterior descrito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "## Con escalares\n",
    "nscalar = 4\n",
    "fscalar = 4.198\n",
    "\n",
    "## Con vectores\n",
    "nvector = np.array([1,22,51])\n",
    "\n",
    "## Con matrices\n",
    "nmatriz = np.array([[3,5,7], [9,12,16]])\n",
    "\n",
    "## Con tensores\n",
    "\n",
    "ntensor = [[[4,5,1,-1]],[[2,4,5,6]],[[9,6,3,1]]] \n",
    "\n",
    "ntensor2 = np.arange(64).reshape((4, 4, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tambien podemos manipular estas variables con operaciones matemáticas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Matriz matemáticamente\n",
    "\n",
    "m = np.array([[-1,2],[3,-4]])\n",
    "n = np.array([[0.3,0.8],[-0.3,-4]])\n",
    "\n",
    "m + n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tomando producto punto (Salida a Escalar)\n",
    "\n",
    "o = np.array([-1,4,-8])\n",
    "p = np.array([-3,1,0])\n",
    "np.dot(o,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizando el producto Hadarmard (Salida a vector)\n",
    "\n",
    "r = np.array([3,5,7])\n",
    "s = np.array([21,31,41])\n",
    "r * s "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribuciones en Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graficando las funciones de densidad probabilistica en python.  En este ejemplo utilizamos un kernel gausiano (KDE), FUNCION DE python de la librería scipy.  Un KDE nos ayuda a estimar la densidad de la función probabilistica.  Gausianos son otra forma de distribución normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.kde import gaussian_kde\n",
    "from numpy import linspace\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = np.random.randn(10000)  ## Crea datos aleatorios; numpy creará datos por nosotros\n",
    "\n",
    "gKDE = gaussian_kde(data)\n",
    "\n",
    "dspace = linspace(min(data), max(data), 100)\n",
    "\n",
    "plt.plot(dspace, gKDE(dspace))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graficando la probabilidad de la función de Mass. Usaremos la data superior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins = 50\n",
    "\n",
    "## Numero de contenedores para la funcion de probabilidad de Mass. \n",
    "cnts, bins = np.histogram(data, bins=nbins)\n",
    "bins = bins[:-1] + (bins[1] - bins[0])/2\n",
    "\n",
    "prob = cnts/float(cnts.sum())\n",
    "\n",
    "## Crear el gráfico\n",
    "plt.bar(bins, prob, 1.0/nbins)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regla de Bayes y Estadísticas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La regla de Bayes nos permite calcular la condición de probabilidad de eventos ocurrientes por medio de invertir las condicioes de eventos.\n",
    "\n",
    "### $$P\\left(\\;A\\;|\\;B\\;\\right) = \\frac{P\\left(\\;B\\;|\\;A\\;\\right)P\\left(\\;A\\;\\right)}{P(\\;B\\;)}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordar que hay dos \"escuelas\" que giran en torno a como ver la estadística: \n",
    "\n",
    "**Frecuencionistas** creen que la distribución normal es fija y no es desconocida.  Podemos ganar mas vistazos a la \"verdadera\" distribución utilizando técnicas de muestreo, efectos para pruebas y estudiando los parametros relativos a la distribución.\n",
    "\n",
    "**Bayesianos** creen que la data nos informa acerca de la distribución y a medida que recibimos más data nuestra vista de la distribuci´pn puede ser actualizada, más alla de confirmar o negar las creencias previas.  Las observaciones bayesianas nunca son \"certeras\".\n",
    "\n",
    "En aplicaciones de IA, siempre vamos a verlo desde el punto de vista bayesiano de las estadísticas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_enfPos = 0.8 ## Probabilidad de tener una enfermedad.  Resultado Positivo.\n",
    "p_enfNeg = 0.2 ## Probabilidad de tener una enfermedad.  Resultado Negativo.\n",
    "\n",
    "p_noPos = 0.096\n",
    "p_noNeg = 0.904\n",
    "\n",
    "p_FalsosPos = (.80 * .01) + (.096 * .99)\n",
    "\n",
    "p_enf_dado_pos = (.80 * .01) / p_FalsosPos\n",
    "\n",
    "print(p_enf_dado_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aprendizaje Supervizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En algoritmos de aprendizaje supervizado se confia en la experiencia previa de las tareas; estas requieren etiquetar la data contenida sobre una variable objetivo.  Por ejemplo, la información suministrada inferiormente es muy famosa para hacer pruebas con aprendizaje supervisado y algoritmos, el Iris Dataset.  Esta nos muestra la información de características de diferentes flores... longitud de sépalo, separación de sépalo, ancho de sépao, ancho de pétalo, longitud de pétalo y ancho de pétalo.  En este dataset, nuestra variable objetivo, usualmente llamada etiqueta es la especie.  Primero carguemos el dataset y veamos como está formado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "irisdata = pd.read_csv(\"iris.csv\")\n",
    "irisdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necesitamos procesar nuestra data codificada por los labels antes de que la alimentemos a la red de aprendizaje.  Utilizaremos un método, que generalmente da buenso resultados llamados RandomForest.  Primero Dividiremos los datasets en datasets de características y etiquetas, luego codificaremos la data antes de ingresarla a nuestro algoritmo utilizando una libreria llamada scikitlearn.  También utilizaremos el labelencoder de scikitlearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "caracteristicas = irisdata.iloc[:,0:4]\n",
    "etiquetas       = irisdata.iloc[:,4]\n",
    "\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(caracteristicas, etiquetas, test_size = 0.25, random_state=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos proceder a hacer \"fit\" que es el entrenamiento de nuestro clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RFclassifier = RandomForestClassifier(n_estimators=1000) \n",
    "RFclassifier.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos como lo hizo el modelo.  Podemos predecir utilizando el conjunto de variables de pruebas utilizando las entradas de pruebas para luego, ya finalmente, retornar estos valores a etiquetas que vamos a decodificar del dataset de iris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicciones = RFclassifier.predict(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente crearemos una matriz de confusión para probar como lo hicimos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(Ytest, predicciones, rownames=['Actual'], colnames=['Predecido'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Búsqueda de hyperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'max_features': [2, 3, 4],\n",
    "    'max_depth': [90, 100, 110, 120],\n",
    "    'min_samples_split': [6, 10, 14],\n",
    "    'min_samples_leaf': [2, 4, 6],   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "busqueda = GridSearchCV(estimator=RFclassifier, param_grid=params, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esto tardará bastante, probar en casa solamente...\n",
    "# ... puede demorar de 15min a horas\n",
    "busqueda.fit(Xtrain, Ytrain)\n",
    "busqueda.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aprendizaje No Supervizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Componentes de Análisis Principal o PCA de sus siglas en inglés es un método de aprendizaje no supervisado para la extracción de características.  Este combina diferentes variables de entrada en una manera de la cual podría darnos las variables que proveen la menor cantidad de información para nosotros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caracteristicas = (caracteristicas - caracteristicas.mean()) / caracteristicas.std()\n",
    "\n",
    "corrMat = np.corrcoef(caracteristicas.values.T)\n",
    "corrMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenVal, eigenVec = np.linalg.eig(corrMat)\n",
    "print(eigenVal)\n",
    "print(eigenVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenpairs = [[eigenVal[i], eigenVec[:,i]] for i in range(len(eigenVec))]\n",
    "eigenpairs.sort(reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proyeccion = np.hstack((eigenpairs[0][1].reshape(eigenVec.shape[1],1), eigenpairs[1][1].reshape(eigenVec.shape[1],1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformar = caracteristicas.dot(proyeccion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "\n",
    "ax = fig.gca()\n",
    "ax = sns.regplot(transformar.iloc[:,0], transformar.iloc[:,1],fit_reg=False, scatter_kws={'s':70}, ax=ax)\n",
    "\n",
    "ax.set_xlabel('componente principal 1', fontsize=10)\n",
    "ax.set_ylabel('componente principal 2', fontsize=10)\n",
    "\n",
    "\n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(12) \n",
    "    \n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(12) \n",
    "    \n",
    "ax.set_title('Componente Principal 1 vs Componente Principal 2\\n', fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Laboratorio - Clasificación de Sitio Segun Censo de US"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el siguiente laboratorio ud utilizará un dataset de censo de los estados unidos reducido a unas cuantas columnas.\n",
    "\n",
    "Su objetivo es estimar dependiendo de las columnas county, stname, census2010 y births2010 saber en que ciudad nació la persona.\n",
    "\n",
    "Los puntos a evaluar serán:\n",
    "\n",
    "- Cargar el dataset (10 pt)\n",
    "- Visualizar los primeros 5 campos (10 pt)\n",
    "- Saber el tamaño de filas y columnas del dataset (10 pt)\n",
    "- Hacer una matriz de correlacion (10 pt)\n",
    "- Limpiar el dataset (no todos los campos son necesarios, solo use los que necesite), (5 pt)\n",
    "- Codificar la data categorica (10 pt)\n",
    "- Entrenar el modelo utilizando estos dos algoritmos:\n",
    " - DecisionTreeClassifier (10 pt)\n",
    " - KNeighborsClassifier (10 pt)\n",
    "- Utilizarl a metrica 'acurracy_score' para evaluar su precisión (10 pt)\n",
    "- Al elegir le mejor algoritmo evaluar las predicciones (10 pt)\n",
    "- Crear una matriz de confusión (5 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
