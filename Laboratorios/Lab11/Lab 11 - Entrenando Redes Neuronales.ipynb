{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 11 - Entrenando Redes Neuronales\n",
    "\n",
    "**Este notebook está basado en el libro Hands-On Machine Learning with scikit-learn and Tensorflow de Aurélien Géron**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anteriormente se vio redes neuronales artificales y su entrenamiento, pero de una manera vaga.  Vimos redes neuronales algo compactas y algunos problemas.  Para problemas más complejos necesitamos redes más profundas y con una gran cantidad de neuronas, pero ayudado a mejorar el rendimiento también se presentan problemas:\n",
    "- Gradientes Explotantes (Exploding Gradients) ó también Gradientes Desvanescientes (Vanishing Gradients), que hacen las redes difíciles de entrenar\n",
    "- ¿Cuánto tiempo es necesario entrenar para que no sea lento?\n",
    "- ¿Cómo ajustamos los hiperparámetros?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# El problema de los gradientes desvanescientes o explotantes\n",
    "\n",
    "Luego de encontrar la salida con feed-forward hacemos backpropagation y dependiendo del gradiente actualizamos los pesos por medio de gradiente descendiente.\n",
    "\n",
    "A veces, los gradientes se van 'desvaneciendo' (haciendose más pequeños) y no actualizando ls capas en donde el algoritmo no mejora el resultado (deja de aprender, se congela la curva de pérdida), a esto se le conoce como gradientes desvanescientes.\n",
    "\n",
    "Por otro lado, a veces estos se vuelvem más grandes haciendo que el algoritmo diverja (haciendose muy grandes, el resultado verá que la curva un 'nan'), esto se llama gradiente explotante.\n",
    "\n",
    "Esto hizo en sus inicios que se abandonara deep learning hasta el 2010 donde X. Glorot y Y. Bengio explicaron el porqué los métodos de activación por sigmoid y el uso de inicialización aleatoria a una distribución normal y 0 varianza causaban estos problemas (la varianza de las capas es mayor a la varianza de las entradas).  Otra manera, a medida que se avanza en la red neuronal la función de activación se statura en las capas superiores y la varianza incrementa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [1, 1], 'k--')\n",
    "plt.plot([0, 0], [-0.2, 1.2], 'k-')\n",
    "plt.plot([-5, 5], [-3/4, 7/4], 'g--')\n",
    "plt.plot(z, logit(z), \"b-\", linewidth=2)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Saturating', xytext=(3.5, 0.7), xy=(5, 1), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Saturating', xytext=(-3.5, 0.3), xy=(-5, 0), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Linear', xytext=(2, 0.2), xy=(0, 0.5), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.grid(True)\n",
    "plt.title(\"Sigmoid activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.2, 1.2])\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Del código anterior note que se satura en los extremos (o entradas muy negativas o muy positivas dan valor de salida 0 ó 1 respetivamente), causando que no se actualicen los gradientes en backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicializaciones Xavier y He"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-da109dac52d3>:3: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From c:\\users\\isaias\\anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow_core\\python\\layers\\core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    }
   ],
   "source": [
    "he_init = tf.variance_scaling_initializer()\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                          kernel_initializer=he_init, name=\"hidden1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el código anterior buscamos entrenar una red neuronal utilizando MNIST dataset, más adelante se verá a detalle.\n",
    "\n",
    "Glorot y Bengio explican que debe ser uniforme el propagar las señales (forward y backpropagation) sin explotar o desvanecerse (obvio) haciendo las varianzas de las capas y entradas idénticas (en forward) y las varianzas de los gradientes sean iguales (en back).  Lo más cercano a lo propuesto, pues es casi imposible lograrlo es inicializar aleatoriamente los pesos, también llamada [Xavier Initialization](https://keras.io/initializers/), en keras, que es un framework para redes neuronales tenemos glorot_normal y glorot_uniform.\n",
    "\n",
    "**Esta inicialización funciona bien para regresión logística.**\n",
    "\n",
    "El resultado de estas inicializaciones aceleran el entrenamiento, no es por nada una de las más utilizadas al realizar experimentos con redes neuronales.  Adicionalmente existen otras estrategias de inicialización como cuando se utiliza Tangente Hiperbólica y ReLU en funciones de activación.\n",
    "\n",
    "En tensorflow por defecto la inicialización es Xavier a menos que se especifique otra (He, funciona bien con ReLU)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de Activación No Saturadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como observamos parte de que las redes neuronales no aprendieran segun el paper de Glorot y Bengio era la mala escogencia de las funciones de activación y sus inicializadores.  Las funciones como ReLU no se saturan a grandes valores y son fáciles de computar.\n",
    "\n",
    "ReLU tiene el problema (a pesar de que su comportamiento es bueno) que las neuronas mueren (probablemente la mitad de su red neuronal de 0 de salida), más cuando el LR (learning rate) es alto.  El 0 se debe a que a resultados negativos la función siempre da 0, para esto crearon una función alterna llamada leaky ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaky ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(z, alpha=0.01):\n",
    "    return np.maximum(alpha*z, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, leaky_relu(z, 0.05), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-0.5, 4.2], 'k-')\n",
    "plt.grid(True)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Leak', xytext=(-3.5, 0.5), xy=(-5, -0.2), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.title(\"Leaky ReLU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.5, 4.2])\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos entonces que la función de ReLU daría 0 para valores negativos, pero la Leaky ReLU tiene una pequeña pendiente a valores negativos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(z, name=None):\n",
    "    return tf.maximum(0.01 * z, z, name=name)\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"hidden1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo anterior podemos ver un resultado de [Leaky ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) en Tensorflow.  Algunos tmbién recomiendan un valor de alpha de 0.2 en vez de 0.01.  Otras funciones de activación proponen también cambiar el hiperparámetro alfa dinamicamente (RReLU) a medida que avanza el entrenamiento y otros de forma paramétrica PReLU, que hace alfa un hiperparámetro que se aprende durante el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=leaky_relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\isaias\\anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El código anterior entrena define una gráfica antes de entrenar la red neuronal basada en MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Batch accuracy: 0.86 Validation accuracy: 0.9048\n",
      "5 Batch accuracy: 0.94 Validation accuracy: 0.9496\n",
      "10 Batch accuracy: 0.92 Validation accuracy: 0.9658\n",
      "15 Batch accuracy: 0.94 Validation accuracy: 0.9712\n",
      "20 Batch accuracy: 1.0 Validation accuracy: 0.9766\n",
      "25 Batch accuracy: 1.0 Validation accuracy: 0.9776\n",
      "30 Batch accuracy: 0.98 Validation accuracy: 0.978\n",
      "35 Batch accuracy: 1.0 Validation accuracy: 0.9786\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if epoch % 5 == 0:\n",
    "            acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "            print(epoch, \"Batch accuracy:\", acc_batch, \"Validation accuracy:\", acc_valid)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El código anterior muestra un entrenamiento del MNIST utilizando la inicialización propuesta por Glorot y Bengio utilizando leaky ReLU como función de activación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elu(z, alpha=1):\n",
    "    return np.where(z < 0, alpha * (np.exp(z) - 1), z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, elu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1, -1], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"ELU activation function ($\\alpha=1$)\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El código anterior muestra la función de actiavción ELU, que según datos comprobados mejoras con creces el desempeño de ReLU reduciendo el tiempo de entrenamiento y desempeñandose mejor en el test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.elu, name=\"hidden1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El código anterior implementa ELU en Tensorflow.\n",
    "\n",
    "ELU a diferencia de ReLU tiene diferencias como:\n",
    "- No se satura a varios valores negativos (para ELU hay una pequeña curva que el valor de alfa está cerca a 1, pero puede cambiarse)\n",
    "- No permite que las neuronas 'mueran'\n",
    "- La función es 'suave' incluso cuando z = 0.\n",
    "- Converge rápidamente\n",
    "\n",
    "Desventajas de ELU:\n",
    "- Más costosa de computar que ReLU y sus variantes\n",
    "- Tiempo de prueba de ELU es menor de la ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import erfc\n",
    "\n",
    "# alpha and scale to self normalize with mean 0 and standard deviation 1\n",
    "# (see equation 14 in the paper):\n",
    "alpha_0_1 = -np.sqrt(2 / np.pi) / (erfc(1/np.sqrt(2)) * np.exp(1/2) - 1)\n",
    "scale_0_1 = (1 - erfc(1 / np.sqrt(2)) * np.sqrt(np.e)) * np.sqrt(2 * np.pi) * (2 * erfc(np.sqrt(2))*np.e**2 + np.pi*erfc(1/np.sqrt(2))**2*np.e - 2*(2+np.pi)*erfc(1/np.sqrt(2))*np.sqrt(np.e)+np.pi+2)**(-1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selu(z, scale=scale_0_1, alpha=alpha_0_1):\n",
    "    return scale * elu(z, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEMCAYAAAAh7MZPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl4FUW+//H3F8KOGgGNOzjjviBq1NHRMePyuPx03HBfLoNKhNErCuMKIyoqLsygIigIoqAigjjKT+e6xnHlGjSuI4sK4oKyGCQhkJDU/aNOzCELIaRP6iyf1/P0k87pTvf3VDrfU6murjLnHCIikp5ahQ5AREQSR0leRCSNKcmLiKQxJXkRkTSmJC8iksaU5EVE0piSvKQ8M1toZoNb4DzDzOzTFjhPKzN7yMyWm5kzs7xEn7OReCaZ2ayQMcimU5JPI2a2lZmNiSW9tWb2o5m9ambHxu1TEEsctZepcfs4M+vdwDn6mFlJA9sa/LkobCDJHgSMifA8PWLvJbfWpnuAI6M6zwacCPwZOBnYFninBc6JmeXF3ne3WpuuBC5oiRgkelmhA5BIzQA6AhcDC4Ct8Umpa639HgFuqPVaWcKjSxDn3NIWOk8JUO8HXMR2AX5wzrVIcm+Mc25l6BikGZxzWtJgAbIBBxzTyH4FwOhG9nFA7wa29QFKmvpzse3HA28CPwMrgP8B9qy1z3bA48ByYDVQBPwxdl5Xa+kT+5mFwODY+pPAjFrHbAUsBq7amDjqOU9B7PVhwKe1jjs0duy1wCfAKXHbe8R+/gzg5dj7+Rw4dgNlNKnWuRc29HuL7Tur1u92DHA7sAz4Cf/fR6u4fdrGti+KxfwV8N9xscYvkxo4TztgFPAjsAZ4Dzg8bnte7OePBmbH3nchcEDov5NMXNRckz6qa5l/MrP2oYNpQCd8cjgYnwhWAs+bWVsAM+sEvIFPOKcB+wK3xH72KWAkMBffhLFt7LXapgD/z8yy4147Mrb/kxsTR+x18B8G2wKnN/B+rgT+Clwbi3Um8IyZ9aq1323AfcB+wPvAVDPrvIFj3gJ8Gzv3QQ3s15DzgXXAYcDlwEDg7LjtjwIXAVcDe+L/6yvGf1CdEdtn79i5r2zgHHfFjtkX2B//4fYvM9u21n53ANcBB+A/tB83M2vi+5HmCv0poyW6Bf9HugJfu3oXX4s7pNY+BUA5NR8K1cuAuH0SUpOvZ/9OQCWxWiBwKbAK6NbA/sOIq0nHvb6Qmpp8Fr4Ge3Hc9oeB/2lCHD1i7yV3Q+cHvgP+Vk/5Tql1nPy47dvHXjt8A/EMJlaDr3XcjanJv1trn5eBh2Pru8bOfXwD582Lbe/W0HliZVUOXBS3vTXwJTC81nGOi9vn97HXdgj9d5Jpi2ryacQ5NwPf3HEy8CK+NveemdVuf38K6FVreTzR8ZnZb83sCTP70sx+wf+73wrYKbbL/sDHzrllm3oO59w6/Ps7P3bOdvgPvylNiGNj3svm+LJ+u9amt4C9ar32cdz697GvW2/suZro41rffx93rv2BKuD1Zhz/t0Ab4t63c64SX6kI+b6lAbrxmmacc2vwtbeXgVvM7GFgmJnd45wrj+220jm3YBNP8QvQwczaOOcqql+Max7Z0E265/G13/zY13X4NurqZpKo/pWfArxjZtsDh8SOP7MJcTRFfcO41n7t13JyzrlYi0VTK1hV1C2fNvXsV1Hrexd3rijKt/oYTXrfcdtUsWxhKvD09zn+wzyqdvq5+Otm/1qvHxC3vQ4z64pvA77dOfeKc+4/wGasX9H4AOhZTxe+auX4poENcs7NxjcfnIuv0T/rfM+YjY2j+sOwwXM5537B104Pr7XpcHyZR20pvp083n5NPMYH+N/dHxvY3uj7xvfaKifufZtZa+BQEvO+pZlUk08TseT1NDAR/2/yKiAXuAZ4NZaUqnU0s21qHaLcObci7vse9dxA/Mo595mZvQQ8bGZX45PpbsC9wDTn3DcNhPgzvsfHpWa2GN82fTe+Fl3tCfyNumfN7Hr8zcd9gVXOudfxbe/dzewA4JvY62sbON/jwCXU3MRtShw/4buUHmdmC4E1rv5uhHfj/1uaD8zB9yU/AjiwgZia4zVglJn9Cf9Bmg/siC+TjeKcm29m0/C/uyvxSX8HoIdzbjK+x43D37h+Hiir/nCMO0apmY0FRpjZMuBr4CoghwifVZAIhb4poCWaBd+t7XZ8742f8d3W5gN/B7rE7VdA3a5yDngrbp/6tjvgpNj2bHxSXxA7zzzgTqBzIzEeBXyKvzH8KXAc/qZvn7h9dsC3qRfHjv0hkBf3HqfH3l+9XSjjjvPb2D4/AlmbEMcl+A+SSjauC2U5vpfJqXHbe1D/DdzGuprWd+O1DfAA/gNqGb4HziTq3nht7OZsO3zvmO/wXSi/BC6P2z4U+AHfPDRpA8eo7kK5loa7UHZrrCy0JH6x2C9ARETSkNrkRUTSmJK8iEgaU5IXEUljSvIiImkseBfKbt26uR49egSNobS0lE6dOgWNIVmoLLy5c+dSWVnJXnvVfogzMyXDdVFaCnPngnOw887QpUuoOMKXBcCcOXOWOee2amy/4Em+R48eFBYWBo2hoKCAvLy8oDEkC5WFl5eXR3FxcfBrM1mEvi5++AEOPNAn+CuvhFGjgoUSvCyqmdmijdlPzTUiktTKy+HMM32i/8Mf4O67Q0eUWpTkRSSpDRoEb78N228P06ZBm/pG7JEGKcmLSNJ67DEYPRratoUZMyAnJ3REqSfSJG9mU8zsBzP7xczmmdklUR5fRDLHBx9Afr5fv/9+OOSQsPGkqqhr8nfgBzvaHPgTMNzMEjFYk4iksWXL4PTTYc0auOQS6NcvdESpK9Ik75z7zNWMClg9qNVvozyHiKS3yko491xYtAgOPtg318imi7wLpZmNwU8R1wE/guAL9ezTD+gHkJOTQ0FBQdRhNElJSUnwGJKFysIrLi6msrJSZRHTktfFuHG/4ZVXdiI7u5xBg+bw7rsNjSYdRqr9jSRkFMq4SQTygDtd3AxCteXm5rrQfZGTpd9rMlBZeNX95IuKikKHkhRa6rqYMQN694bWreGVVyAZL8Vk+RsxsznOudzG9ktI7xrnXKVz7i382OD9E3EOEUkvn38Offr49bvvTs4En4oS3YUyC7XJi0gjVq6E006DkhLfHj9wYOiI0kdkSd7Mtjazc8yss5m1NrPj8HNsvhbVOUQk/VRVwUUXwbx50LMnjB8PFtWU7hLpjVeHb5p5EP/hsQgY6Jz7Z4TnEJE0c9tt8NxzkJ0NzzwDSTD2V1qJLMk755YCR0Z1PBFJfy+8ADfd5GvuTzwBv1XjbuSCj0IpIplpwQI4/3w/suStt8IJJ4SOKD1p7BoRaXGlpf6J1uJiOOUUuOGG0BGlLyV5EWlRzvmhCj75BHbbDR59FFopEyWMilZEWtSoUTB1KnTuDM8+C1tsETqi9KYkLyIt5vXX4a9/9euPPgp77hk2nkygJC8iLWLxYjj7bD8A2XXX+TZ5STwleRFJuDVr4IwzYOlSOPZYGD48dESZQ0leRBLKObj8cnj/fejRA5580g9AJi1DSV5EEmr8eJgwAdq390+0du0aOqLMoiQvIgnz3nu+Fg8wbhzsv3/YeDKRkryIJMSSJb4dvqICrrgCLrwwdESZSUleRCJXUQFnnQXffw9HHAEjR4aOKHMpyYtI5AYPhjffhO22g2nToE2b0BFlLiV5EYnUlClw330+sU+fDttsEzqizKYkLyKRKSqCfv38+n33waGHho1HlORFJCIrVvgp/MrKoG9fyM8PHZGAkryIRKCy0s/NunAh5ObCAw9oCr9koSQvIs02dCi89BJ06wYzZvgHnyQ5KMmLSLM88wzccYcfE37aNNhpp9ARSTwleRHZZP/5D/zXf/n1u+6CP/4xbDxSl5K8iGySX37xN1pLSvwQwldfHToiqY+SvIg0WVWVr8HPnQv77OMHINON1uSkJC8iTTZihJ+6LzsbZs6ETp1CRyQNUZIXkSb5179gyBBfc3/8cdhll9ARyYZkhQ5ARFLHV1/Beef5iUBuvhlOPDF0RNIY1eRFZKOsXu1vtP78M5x8sq/NS/JTkheRRjkHl14KH38Mu+4Kkyf7fvGS/PRrEpFGzZixPU884W+wzpwJW2wROiLZWEryIrJBb7wBY8f6u6uPPAJ77x04IGkSJXkRadC33/oZnqqqjGuugTPPDB2RNJWSvIjUa+1aP0frTz/BgQeu4LbbQkckmyKyJG9m7cxsgpktMrNVZvahmZ0Q1fFFpGVdcQX87/9C9+4wdOh/yFKH65QUZU0+C1gMHAlsAQwFpplZjwjPISItYPx4v7Rv70eZ3GKLitAhySaKLMk750qdc8Occwudc1XOuVnA18CBUZ1DRBJv9my4/HK//uCDcMABYeOR5knYP2BmlgPsBnxWz7Z+QD+AnJwcCgoKEhXGRikpKQkeQ7JQWXjFxcVUVlZmXFmsWNGG/Pxcysvbceqp39G9+3wKCnRdxEu1sjDnXPQHNWsDvAh86Zzb4EyPubm5rrCwMPIYmqKgoIC8vLygMSQLlYWXl5dHcXExRUVFoUNpMRUVcOyxvsvk738Pr70Gbdv6bbouaiRLWZjZHOdcbmP7Rd67xsxaAZOBcuDyqI8vIolxzTU+wW+7LTz9dE2Cl9QWaXONmRkwAcgBTnTO6W6NSAp44gkYNQratIHp032il/QQdZv8WGBP4BjnXFnExxaRBPjoI7jkEr8+ahQcdljYeCRaUfaT7w7kA72AJWZWElvOj+ocIhKtFSv8yJJlZdCnD/TvHzoiiVpkNXnn3CJAE4CJpIjKSjj/fPj6a99NcswYTeGXjjSsgUiGGjbMz/LUrZt/4KlDh9ARSSIoyYtkoGefheHD/ZjwU6f6oQskPSnJi2SYL76Aiy7y6yNGwNFHh41HEktJXiSDrFrlb7SuWuWHDR48OHREkmhK8iIZwjnfg+aLL/zEHxMn6kZrJlCSF8kQd95ZPaKkn8Kvc+fQEUlLUJIXyQAvvQQ33ujXp0zxk3FLZlCSF0lzX38N554LVVVw001w0kmhI5KWpCQvksZWr4bTT/dPtp50Evztb6EjkpamJC+SppyD/HwoKoJddoHJk32/eMks+pWLpKnRo337e8eO/kZrdnboiCQEJXmRNPTmm3D11X79kUdgn33CxiPhKMmLpJnvvvMPOq1b5x92Ouus0BFJSEryImlk7Vro3Rt+/BGOOgruuCN0RBKakrxIGrnySnjvPdhpJz/wWFbU0wJJylGSF0kTEybAQw9Bu3YwYwZstVXoiCQZKMmLpIH334cBA/z62LGQmxs2HkkeSvIiKe6nn/wDT+Xlfvq+P/85dESSTJTkRVLYunVw9tnw7bdw6KF+Im6ReEryIinsuuugoAC22QamT4e2bUNHJMlGSV4kRU2dCiNH+h40Tz8N220XOiJJRkryIino44/h4ov9+j/+AYcfHjYeSV5K8iIp5uef/Y3W1av9XK1/+UvoiCSZKcmLpJCqKrjgAvjyS9h/f3jwQU3hJxumJC+SQm6+GV54Abp29VP5degQOiJJdkryIiniuefgllv8mPBPPgk9eoSOSFKBkrxICpg3Dy680K/ffjsce2zYeCR1KMmLJLlVq+C00+CXX+CMM+Caa0JHJKlESV4kiTkHffvC55/DXnv5CUB0o1WaQkleJIndfbd/knXzzf2N1s02Cx2RpJpIk7yZXW5mhWa21swmRXlskUzz8stw/fV+ffJk2H33sPFIaop6SoHvgeHAcYA6d4lsooUL4dxzfb/4oUPhT38KHZGkqkiTvHPuGQAzywV2iPLYIpmirMw/0bp8OZx4IgwbFjoiSWVBJgczs35AP4CcnBwKCgpChPGrkpKS4DEkC5WFV1xcTGVlZYuXhXMwYsQefPjhNmy3XRmXXTaHf/97XYvGUB9dFzVSrSyCJHnn3DhgHEBubq7Ly8sLEcavCgoKCB1DslBZeNnZ2RQXF7d4WTzwALz0EnTsCC++2IGePZNj5DFdFzVSrSzUu0YkSbz1Fgwc6NcnTICePcPGI+lBSV4kCXz/PZx5pp/p6eqr4ZxzQkck6SLS5hozy4odszXQ2szaA+ucc+EbFUWSVHm5T/BLlkBeHtx5Z+iIJJ1EXZMfApQB1wEXxNaHRHwOkbRy1VXwzjuwww7w1FN+pieRqETdhXIYMCzKY4qks0mTYMwYPzfrM8/A1luHjkjSjdrkRQIpLITLLvPrY8bAQQeFjUfSk5K8SABLl/oHntauhfz8mvlaRaKmJC/Swtat871nFi+G3/0O7r03dESSzpTkRVrYDTfAa69BTo4fYbJdu9ARSTpTkhdpQdOm+eGDs7Lg6adh++1DRyTpTklepIV8+qmfAARg5Eg44oiw8UhmUJIXaQHFxX4Kv9JSuOACuOKK0BFJplCSF0mwqio/CfeCBdCrFzz0kKbwk5ajJC+SYLfeCrNmwZZb+geeOnYMHZFkEiV5kQSaNctP+mEGTz4JO+8cOiLJNEryIgkyf75vfwe47TY47riw8UhmUpIXSYCSEn+jdeVK//W660JHJJlKSV4kYs75YQo++wz22MMPQqYbrRKKkrxIxEaO9A89bbYZzJwJm28eOiLJZEryIhF69VW49lq//thjviYvEpKSvEhEFi2Cs8/2/eJvvBFOPTV0RCJK8iKRKCuDM86A5cvh+OPh5ptDRyTiKcmLNJNzMGAAzJkDv/kNPP44tG4dOioRT0lepJkefND3oOnQwT/R2qVL6IhEaijJizTDO+/AlVf69Ycfhv32CxuPSG1K8iKb6IcfoHdvqKiAgQPhvPNCRyRSl5K8yCYoL4czz/SJ/sgj4a67QkckUj8leZFNMGgQvP22n9npqaegTZvQEYnUT0lepIkeewxGj4a2bWHGDD9Xq0iyUpIXaYIPPoD8fL8+ejQcckjYeEQaoyQvspGWLYPTT4c1a+DSS/0ikuyU5EU2wrp1cO65fuiCgw+G++8PHZHIxlGSF9kIQ4bAK6/A1lv7dvh27UJHJLJxlORFGjF9Otx5px+qYNo02GGH0BGJbDwleZEN+Pxz6NPHr99zj+8TL5JKIk3yZtbFzGaaWamZLTIzPQMoKauy0jj1VCgt9U+zVg9fIJJKsiI+3gNAOZAD9AL+v5l95Jz7LOLziCTc4sUdWbkSevaE8eM1hZ+kJnPORXMgs07Az8A+zrl5sdcmA9855xqcxnizzTZzBx54YCQxbKri4mKys7ODxpAsVBZeYWERpaXQunUvcnOhffvQEYWl66JGspTFG2+8Mcc5l9vYflHW5HcDKqsTfMxHQJ1WTDPrB/QDaNOmDcXFxRGG0XSVlZXBY0gWKguvrMwBxlZbrWHNmjWsWRM6orB0XdRItbKIMsl3BlbWem0lsFntHZ1z44BxALm5ua6wsDDCMJquoKCAvLy8oDEkC5UFPP00nHVWHllZVSxY8G86dQodUXi6LmokS1nYRrYfRnnjtQSoPS/95sCqCM8hklAVFX5+VoBttlmrBC8pL8okPw/IMrNd417bD9BNV0kZEyfC/Pl+lqcuXdaGDkek2SJL8s65UuAZ4BYz62RmvwdOASZHdQ6RRCotrZmAe+ed1ZtG0kPUD0MNADoAPwFPAv3VfVJSxb33+klAcnNhq61CRyMSjUiTvHNuhXPuVOdcJ+fcTs65J6I8vkiiLF/uhy4AGDEibCwiUdKwBiLAHXfAL7/AscfC0UeHjkYkOkrykvEWLvQTgIBq8ZJ+lOQl4113Haxd68enOeCA0NGIREtJXjLau+/6ibjbt/dNNiLpRkleMlZVFVx1lV8fPBh22ilsPCKJoCQvGeupp2D2bNhmG7j22tDRiCSGkrxkpLIy3xYPMHw4dO4cNh6RRFGSl4w0YgR8840fK7565ieRdKQkLxnniy9qukref7+fu1UkXSnJS0ZxDi67DMrL4eKL4Q9/CB2RSGIpyUtGeewxeOMN6NatZhgDkXSmJC8ZY9kyGDTIr//979C1a9h4RFqCkrxkjEGD/EBkRx0FF1wQOhqRlqEkLxlh5kzfVNO+PYwdq7HiJXMoyUva+/FH6NfPr991F+y2W9h4RFqSkrykNefg0kt9e/zRR8Nf/hI6IpGWpSQvae2RR+D552GLLfx6K13xkmF0yUvamjcPrrzSr48eDTvuGDYekRCU5CUtrV4NvXtDSQmcdRacf37oiETCUJKXtHT55fDJJ7DrrjB+vHrTSOZSkpe088gjfmnfHqZPh803Dx2RSDhK8pJWPvoIBgzw62PG+FEmRTKZkrykjSVL4OSTYc0a+POf/SKS6ZTkJS2UlcGpp8LixXDoob4WLyJK8pIGnPO19tmzoXt3ePZZ3x4vIkrykgZuusnP17rZZjBrFmy9deiIRJKHkryktPvvh1tv9U+yTp0K++wTOiKR5KIkLylr8mT47//26+PHw4knho1HJBkpyUtKeu65mt4z99wDffuGjUckWSnJS8p58UU/VEFlJdx4Y81sTyJSVyRJ3swuN7NCM1trZpOiOKZIff75TzjlFFi7Fq64wrfHi0jDoqrJfw8MByZGdDyROqZN84OOVVTAVVfBvfdqTBqRxkSS5J1zzzjnngWWR3E8kdomToRzz4V16+D662HkSCV4kY2RFeKkZtYP6AeQk5NDQUFBiDB+VVJSEjyGZJFsZeEcPPpoDx59tAcAffp8zbHHLuKNNxJ73uLiYiorK5OqLEJKtusipFQriyBJ3jk3DhgHkJub6/Ly8kKE8auCggJCx5AskqksKiogPx8efdT3gx89Gvr33xnYOeHnzs7Opri4OGnKIrRkui5CS7WyaLS5xswKzMw1sLzVEkFK5lm+3Pd7f+QR6NjRD1XQv3/oqERST6M1eedcXgvEIfKrDz+E00+HhQv9EAWzZsFBB4WOSiQ1RdWFMsvM2gOtgdZm1t7MgjQFSWqbMgUOO8wn+IMOgsJCJXiR5oiqC+UQoAy4Drggtj4komNLBigpgYsvhgsv9OPB9+0L//63Jt8Waa5IatvOuWHAsCiOJZlnzhzfPXL+fGjXzvd/79dPXSRFoqBhDSSYigq47TY/ycf8+X4EycJC36NGCV4kGmo3lyA++MA3zxQV+e+vuALuvBM6dAgbl0i6UU1eWlRJCVx7LRx8sE/wO+8ML78M992nBC+SCEry0iKcgyeegN13h7vugqoqP/7MJ5/AMceEjk4kfam5RhLuvffgr3+Ft2KPzh10kH969eCDw8YlkglUk5eE+ewzOO00f2P1rbf8g00TJ/qkrwQv0jJUk5fIffyxv4k6dapvlunYEQYOhGuugS22CB2dSGZRkpfIvPkmjBgBL7zgv8/KgssugyFDYNttw8YmkqmU5KVZKirg+efh73+Ht9/2r3XoAJdeCldfDd27h41PJNMpycsmWbQIxo+HCRNgyRL/2pZb+v7uV1wB3bqFjU9EPCV52WhlZb4pZuJEP5m2c/71Pff0zTJ9+0LnzmFjFJH1KcnLBlVUwKuvwpNPwsyZsGqVf71tWz/fan4+HHGEhiEQSVZK8lJHaalP7M8/7yfrWLasZltuLpx3nh8tUk0yIslPSV4A+Ppr3wTz2GP7UlQEa9fWbNtzTz9K5DnnwK67hotRRJpOST5DffcdvP66X157zU/S4XXFDA45BE4+2S/77qvmGJFUpSSfAdau9YOBzZ5ds3z55fr7bLklHHUU7LLLF1x11R7k5ISJVUSipSSfZsrK4PPP/cBfH37oE/qHH0J5+fr7de4Mf/iDT+xHHQU9e0Lr1lBQsIScnD3CBC8ikVOST1GrV/va+Lx58OmnPql/8gksWOCHEqhtzz19E8zvfue/7rOPfyJVRNKb/syTlHOwdCl88w0sXuyT94IFfgal+fPh22/r/7nWrWGvvXw7es+efiCwgw7SmDEimUpJPoCyMvjxx5plyRKfyKsTevUS38OltqwsP+HGrrvC3nv7hL7vvrDHHn6eVBERUJJvFuf8w0E//wwrVqz/NX596VL46aeapF79QFFjttwSdtoJdtyxJqFXL927q7lFRBqX9mmiogLWrPFLWdn6X6vXCwu78cMPvp27pMQn4eqv8eu1v65cCZWVTY+pTRvIyfHjq+fk+KU6mVd/3XFHDREgIs0XPMn/8AP87W8+GTd3KS/3S3wi37gkvM8mx9+pE3Tp4mvd1Uv89126QNeuNck8Jweys9XvXERaRvAk//33c7n11rxar54FDABWAyfW81N9YssyoHc92/sDZwOLgQtp1Yr1lpycQWy99clUVc3lq6/yqaysoF27NrRq5ZtAjjhiCD17HsPKlUU8++xAWrf2NzSzsvzXa6+9nby8w/jss3e46aYb1jvzzz/DTTeNolevXrzyyisMHz68TnQPPfQQu+++O88//zwjR46ss33y5MnsuOOOPPXUU4wdO7bO9unTp9OtWzcmTZrEpEmT6mx/4YUX6NixI2PGjGHatGl1thcUFABwzz33MGvWrPW2lZWVMXv2bABuvfVWXn311fW2d+3alRkzZgBw/fXX8+677663fYcddmDKlCkADBw4kKKiovW277bbbowbNw6Afv36MW/evPW29+rVi1GjRgFwwQUX8G2tO8yHHnood9xxBwBnnHEGy5cvX2/70UcfzdChQwE44YQTKCsrW2/7SSedxODBgwHIy8ujtrPOOosBAwZQVVXFggUL6uzTp08f+vTpw7Jly+jdu+61179/f84++2wWL17MhRdeWGf7oEGDOPnkk5k7dy75+fl1tg8ZMoRjjjmGoqIiBg4cWGf77bffzmGHHcY777zDDTfcUGf7qFGJufaKi4vJzs5O6LXXoUMHXnzxRSCzr73Vq1dz4ol1815j115Dgif5tm39hBKtWvnarRkceCAcfbTvCnjvvf61+O3HHw8nneTHWBkyZP3trVr50RDPOce3hfftW/ecgwb5JznnzvUDbBUXl5Kdnf3r9osv9pNLFxX5qepq2247P25LmzYJLBgRkQiYqx4vNpDc3FxXWFgYNIaCgoJ6P1kzkcrCy8vLo7i4uE5tMFPpuqiRLGVhZnOcc7mN7aewUZPsAAADlklEQVSJvEVE0piSvIhIGlOSFxFJY0ryIiJpTEleRCSNNTvJm1k7M5tgZovMbJWZfWhmJ0QRnIiINE8UNfks/FNHRwJbAEOBaWbWI4Jji4hIMzT7YSjnXCkwLO6lWWb2NXAgsLC5xxcRkU0X+ROvZpYD7AZ8toF9+gH9AHJycn591DmUkpKS4DEkC5WFV1xcTGVlpcoiRtdFjVQri0ifeDWzNsCLwJfOuboDc9RDT7wmF5WFpyde16frokaylEVkT7yaWYGZuQaWt+L2awVMBsqBy5sVvYiIRKLR5hrnXF5j+5iZAROAHOBE51xF80MTEZHmiqpNfiywJ3CMc66ssZ1FRKRlRNFPvjuQD/QClphZSWw5v9nRiYhIs0TRhXIRoHmORESSkIY1EBFJY8EnDTGzpcCioEFAN/xcgqKyiKeyqKGyqJEsZdHdObdVYzsFT/LJwMwKN6a/aSZQWdRQWdRQWdRItbJQc42ISBpTkhcRSWNK8t640AEkEZVFDZVFDZVFjZQqC7XJi4ikMdXkRUTSmJK8iEgaU5IXEUljSvL1MLNdzWyNmU0JHUsImT5vr5l1MbOZZlYaK4PzQscUQqZfBw1JtfygJF+/B4D3QwcRUKbP2/sAfl6EHOB8YKyZ7R02pCAy/TpoSErlByX5WszsHKAYeDV0LKE450qdc8Occwudc1XOuVlA9by9ac3MOgFnAEOdcyXOubeA54ALw0bW8jL5OmhIKuYHJfk4ZrY5cAswKHQsyWRj5u1NI7sBlc65eXGvfQRkYk1+PRl2HdSRqvlBSX59twITnHOLQweSLGLz9j4OPOqc+yJ0PC2gM7Cy1msrgc0CxJI0MvA6qE9K5oeMSfKNzVVrZr2AY4B/hI410TRv7waVAJvXem1zYFWAWJJChl4H60nl/BDV9H9Jr7G5as1sINAD+MZPWUtnoLWZ7eWcOyDhAbYgzdu7QfOALDPb1Tk3P/bafmRuE0WmXge15ZGi+UHDGsSYWUfWr8ENxv9S+zvnlgYJKiAzexA/peMxzrmS0PG0JDObCjjgEnwZvAAc5pzLuESfyddBvFTODxlTk2+Mc241sLr6ezMrAdYk+y8wEeLm7V2Ln7e3elO+c+7xYIG1nAHAROAnYDn+DzkTE3ymXwe/SuX8oJq8iEgay5gbryIimUhJXkQkjSnJi4ikMSV5EZE0piQvIpLGlORFRNKYkryISBpTkhcRSWP/B8U9bOqbCl9qAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, selu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1.758, -1.758], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"SELU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función de activación data del 2017 en el [paper](https://arxiv.org/pdf/1706.02515.pdf) de Günter Klambauer, Thomas Unterthiner y Andreas Mayr. \n",
    "\n",
    "Para este recomiendan si se usa SELU la función de inicialización de LeCun que sera auto normalizada manteniendo la misma varianza y medias durante entrenamiento resolviendo el problema de gradientes explotantes o desvanescientes, especialmente se recomienda esto en redes neuronales.\n",
    "\n",
    "El problema es que SELU se rompe, no se puede unar ni regularizaciones L1 o L, dropout, max-norm, etc., pero funciona bien con CNNs secuenciales. Sin rombper auto-normalización, SELU funciona muy bien."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por defecto, estos hiperparámetros de SELU son ajustados para que las salidas sean cercanas a 0 (hiperparámetros `scale` y `alpha`), y la desviación estándar cerca a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: mean -0.00, std deviation 1.00\n",
      "Layer 100: mean 0.02, std deviation 0.96\n",
      "Layer 200: mean 0.01, std deviation 0.90\n",
      "Layer 300: mean -0.02, std deviation 0.92\n",
      "Layer 400: mean 0.05, std deviation 0.89\n",
      "Layer 500: mean 0.01, std deviation 0.93\n",
      "Layer 600: mean 0.02, std deviation 0.92\n",
      "Layer 700: mean -0.02, std deviation 0.90\n",
      "Layer 800: mean 0.05, std deviation 0.83\n",
      "Layer 900: mean 0.02, std deviation 1.00\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "Z = np.random.normal(size=(500, 100)) # standardized inputs\n",
    "for layer in range(1000):\n",
    "    W = np.random.normal(size=(100, 100), scale=np.sqrt(1 / 100)) # LeCun initialization\n",
    "    Z = selu(np.dot(Z, W))\n",
    "    means = np.mean(Z, axis=0).mean()\n",
    "    stds = np.std(Z, axis=0).mean()\n",
    "    if layer % 100 == 0:\n",
    "        print(\"Layer {}: mean {:.2f}, std deviation {:.2f}\".format(layer, means, stds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selu(z, scale=alpha_0_1, alpha=scale_0_1):\n",
    "    return scale * tf.where(z >= 0.0, z, alpha * tf.nn.elu(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para usar Dropout con SELU debe utilizarse otra tipo de función `tf.contrib.nn.alpha_dropout()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=selu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=selu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "n_epochs = 40\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Del código anterior creamos una gráfica de TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Batch accuracy: 0.94 Validation accuracy: 0.9388\n",
      "5 Batch accuracy: 0.98 Validation accuracy: 0.9634\n",
      "10 Batch accuracy: 1.0 Validation accuracy: 0.9694\n",
      "15 Batch accuracy: 1.0 Validation accuracy: 0.9706\n",
      "20 Batch accuracy: 1.0 Validation accuracy: 0.969\n",
      "25 Batch accuracy: 1.0 Validation accuracy: 0.9692\n",
      "30 Batch accuracy: 1.0 Validation accuracy: 0.971\n",
      "35 Batch accuracy: 1.0 Validation accuracy: 0.9704\n"
     ]
    }
   ],
   "source": [
    "means = X_train.mean(axis=0, keepdims=True)\n",
    "stds = X_train.std(axis=0, keepdims=True) + 1e-10\n",
    "X_val_scaled = (X_valid - means) / stds\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            X_batch_scaled = (X_batch - means) / stds\n",
    "            sess.run(training_op, feed_dict={X: X_batch_scaled, y: y_batch})\n",
    "        if epoch % 5 == 0:\n",
    "            acc_batch = accuracy.eval(feed_dict={X: X_batch_scaled, y: y_batch})\n",
    "            acc_valid = accuracy.eval(feed_dict={X: X_val_scaled, y: y_valid})\n",
    "            print(epoch, \"Batch accuracy:\", acc_batch, \"Validation accuracy:\", acc_valid)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final_selu.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos una red neuronal.\n",
    "\n",
    "Antes debemos de escalar las entradas a 0 media y 1 de desviación estándar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalización del Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ELU y demás funciones al sufrir de saturación no cumplen a veces con regresar a actualizar los pesos.  Para esto se propuso una técnica llamada 'batch normalization' para alivianar los gradientes explotantes o desvanescientes y el cambio de la distribución cambie durante el training.\n",
    "\n",
    "Consiste en antes de realizar la activación de cada capa centrar a 0 y normalizar entradas, ver [algoritmo](https://en.wikipedia.org/wiki/Batch_normalization).  Prácticamente necesita la media y desviación estándar del conjunto de mini batch antes de realizar una adecuación.\n",
    "\n",
    "Se demostró que con varias funciones de activación (que tienen punto de saturación) mejoraba el training eficientemente y eran menos sensitivas a inicialización de pesos, se podían utilizar learning rates más altos.  Al momento de entrenamiento verémos que se vuelve más lento debido al cálculo que debe de hacer pero esto es normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-33-2f36d39a8af9>:15: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=0.9)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=0.9)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "logits = tf.layers.batch_normalization(logits_before_bn, training=training,\n",
    "                                       momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función de tensorflow .batch_normalization() hace el trabajo anteriormente mencionado de normalizr las entradas y centrar en cero.\n",
    "\n",
    "El código anterior define un placeholer del training set ajustada en false para en el training por medio de batch normalization hacer el trabajo de normalizar y centrar cuando se pone en true, luego ubicamos redes totalmente conectadas y al final realizamos una normalización para luego usar la función de activación ELU, note que el hiperparámetro momentum viene dado por $v = v * momentum + v * (1 - momentum)$\n",
    "\n",
    "En la figura inferior tomaremos la función `partial()` para no repetir los mismos parámetros una y otra vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "my_batch_norm_layer = partial(tf.layers.batch_normalization,\n",
    "                              training=training, momentum=0.9)\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "bn1 = my_batch_norm_layer(hidden1)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "bn2 = my_batch_norm_layer(hidden2)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "logits = my_batch_norm_layer(logits_before_bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "batch_norm_momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    he_init = tf.variance_scaling_initializer()\n",
    "\n",
    "    my_batch_norm_layer = partial(\n",
    "            tf.layers.batch_normalization,\n",
    "            training=training,\n",
    "            momentum=batch_norm_momentum)\n",
    "\n",
    "    my_dense_layer = partial(\n",
    "            tf.layers.dense,\n",
    "            kernel_initializer=he_init)\n",
    "\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    bn1 = tf.nn.elu(my_batch_norm_layer(hidden1))\n",
    "    hidden2 = my_dense_layer(bn1, n_hidden2, name=\"hidden2\")\n",
    "    bn2 = tf.nn.elu(my_batch_norm_layer(hidden2))\n",
    "    logits_before_bn = my_dense_layer(bn2, n_outputs, name=\"outputs\")\n",
    "    logits = my_batch_norm_layer(logits_before_bn)\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El código anterior es lo mismo que el codigo que conversamos en el laboratorio pasado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.8952\n",
      "1 Validation accuracy: 0.9202\n",
      "2 Validation accuracy: 0.9318\n",
      "3 Validation accuracy: 0.9422\n",
      "4 Validation accuracy: 0.9468\n",
      "5 Validation accuracy: 0.954\n",
      "6 Validation accuracy: 0.9568\n",
      "7 Validation accuracy: 0.96\n",
      "8 Validation accuracy: 0.962\n",
      "9 Validation accuracy: 0.9638\n",
      "10 Validation accuracy: 0.9662\n",
      "11 Validation accuracy: 0.9682\n",
      "12 Validation accuracy: 0.9672\n",
      "13 Validation accuracy: 0.9696\n",
      "14 Validation accuracy: 0.9706\n",
      "15 Validation accuracy: 0.9704\n",
      "16 Validation accuracy: 0.9718\n",
      "17 Validation accuracy: 0.9726\n",
      "18 Validation accuracy: 0.9738\n",
      "19 Validation accuracy: 0.9742\n"
     ]
    }
   ],
   "source": [
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run([training_op, extra_update_ops],\n",
    "                     feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El código anterior es igual con algunas diferencias, por ejemplo, se hace True la sección que depende de batch normalization-  UPDATE_OPS es necesaria para cuando se da el uso de la función batch_normalization pues debe evaluar algunas alternativas en cada etapa.\n",
    "\n",
    "Notará que no tiene la 'gran precision' mencionada, pero es debido a que para que mejore necesita:\n",
    "- Entrenar por más tiempo\n",
    "- Hacer la red neuronal más profunda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede hacer la función de training dependiente de UPDATE_OPS como sigue:\n",
    "\n",
    "```python\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(extra_update_ops):\n",
    "        training_op = optimizer.minimize(loss)\n",
    "```\n",
    "\n",
    "Así solo se evalua `training_op` durante entrenamiento, TensorFlow automáticamente corre el grupo en la sesión como sigue:\n",
    "\n",
    "```python\n",
    "sess.run(training_op, feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además, note que la lista de variables entrenadas es más corta que la lista de variables globales. Esto se debe a que los promedios móviles no son variables de entrenamiento.  Si se desea utilizar una red neuronal pre-entrenada no se deben de tomar a la ligera el olvidar estas variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hidden1/kernel:0',\n",
       " 'hidden1/bias:0',\n",
       " 'batch_normalization/gamma:0',\n",
       " 'batch_normalization/beta:0',\n",
       " 'hidden2/kernel:0',\n",
       " 'hidden2/bias:0',\n",
       " 'batch_normalization_1/gamma:0',\n",
       " 'batch_normalization_1/beta:0',\n",
       " 'outputs/kernel:0',\n",
       " 'outputs/bias:0',\n",
       " 'batch_normalization_2/gamma:0',\n",
       " 'batch_normalization_2/beta:0']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.name for v in tf.trainable_variables()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hidden1/kernel:0',\n",
       " 'hidden1/bias:0',\n",
       " 'batch_normalization/gamma:0',\n",
       " 'batch_normalization/beta:0',\n",
       " 'batch_normalization/moving_mean:0',\n",
       " 'batch_normalization/moving_variance:0',\n",
       " 'hidden2/kernel:0',\n",
       " 'hidden2/bias:0',\n",
       " 'batch_normalization_1/gamma:0',\n",
       " 'batch_normalization_1/beta:0',\n",
       " 'batch_normalization_1/moving_mean:0',\n",
       " 'batch_normalization_1/moving_variance:0',\n",
       " 'outputs/kernel:0',\n",
       " 'outputs/bias:0',\n",
       " 'batch_normalization_2/gamma:0',\n",
       " 'batch_normalization_2/beta:0',\n",
       " 'batch_normalization_2/moving_mean:0',\n",
       " 'batch_normalization_2/moving_variance:0']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.name for v in tf.global_variables()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clipping del Gradiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preferiblemente se utiliza esta técnica para RNN (Recurrent Neural Networks) pero las personas prefieren batch normalization, ambas previenen gradientes explotantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 50\n",
    "n_hidden5 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"hidden5\")\n",
    "    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El código anterior solamente hace una red neuronal de varias capas ocultas sobre el MNIST dataset con LR = 0.01.\n",
    "\n",
    "Para utilizar 'gradient clipping', se deben computar los gradientes y luego utilizar `clip_by_value()`, finalmente aplicarlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1.0\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
    "              for grad, var in grads_and_vars]\n",
    "training_op = optimizer.apply_gradients(capped_gvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Más de lo mismo a nivel inferior..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.2878\n",
      "1 Validation accuracy: 0.794\n",
      "2 Validation accuracy: 0.88\n",
      "3 Validation accuracy: 0.9062\n",
      "4 Validation accuracy: 0.9162\n",
      "5 Validation accuracy: 0.922\n",
      "6 Validation accuracy: 0.9292\n",
      "7 Validation accuracy: 0.9356\n",
      "8 Validation accuracy: 0.9384\n",
      "9 Validation accuracy: 0.9416\n",
      "10 Validation accuracy: 0.9456\n",
      "11 Validation accuracy: 0.9468\n",
      "12 Validation accuracy: 0.9476\n",
      "13 Validation accuracy: 0.9534\n",
      "14 Validation accuracy: 0.9568\n",
      "15 Validation accuracy: 0.9566\n",
      "16 Validation accuracy: 0.9578\n",
      "17 Validation accuracy: 0.959\n",
      "18 Validation accuracy: 0.962\n",
      "19 Validation accuracy: 0.9616\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reutilizando Capas Pre-Entrenadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando se utiliza redes neuronales no se inicia de cero siempre, es buena costumbre utilizar una red neuronal que hizo una tarea similar y solamente reutilizar las capas infoeriores, esta técnica se llama 'transfer learning'.  Mejora el enmtrenamiento y también tardará menos.\n",
    "\n",
    "Resumen:  En transfer learning solamente desbloqueamos la última capa de pesos y salida y mantenemos fijas las capas con pesos anteriores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusando un Modelo de TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero importamos la gráfica con `import_meta_graph()`, carga la gráfica y la retorna en la variable `Saver`, la gráfica está salvada en este archivo `.meta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.import_meta_graph(\"./my_model_final.ckpt.meta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ver las operaciones en la estructura de gráfica podemos observarla de esta manera.  Esto se hace cuando el modelo no está bien estructurado, pero existe la otra manera con tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\n",
      "y\n",
      "hidden1/kernel/Initializer/random_uniform/shape\n",
      "hidden1/kernel/Initializer/random_uniform/min\n",
      "hidden1/kernel/Initializer/random_uniform/max\n",
      "hidden1/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden1/kernel/Initializer/random_uniform/sub\n",
      "hidden1/kernel/Initializer/random_uniform/mul\n",
      "hidden1/kernel/Initializer/random_uniform\n",
      "hidden1/kernel\n",
      "hidden1/kernel/Assign\n",
      "hidden1/kernel/read\n",
      "hidden1/bias/Initializer/zeros\n",
      "hidden1/bias\n",
      "hidden1/bias/Assign\n",
      "hidden1/bias/read\n",
      "dnn/hidden1/MatMul\n",
      "dnn/hidden1/BiasAdd\n",
      "dnn/hidden1/Relu\n",
      "hidden2/kernel/Initializer/random_uniform/shape\n",
      "hidden2/kernel/Initializer/random_uniform/min\n",
      "hidden2/kernel/Initializer/random_uniform/max\n",
      "hidden2/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden2/kernel/Initializer/random_uniform/sub\n",
      "hidden2/kernel/Initializer/random_uniform/mul\n",
      "hidden2/kernel/Initializer/random_uniform\n",
      "hidden2/kernel\n",
      "hidden2/kernel/Assign\n",
      "hidden2/kernel/read\n",
      "hidden2/bias/Initializer/zeros\n",
      "hidden2/bias\n",
      "hidden2/bias/Assign\n",
      "hidden2/bias/read\n",
      "dnn/hidden2/MatMul\n",
      "dnn/hidden2/BiasAdd\n",
      "dnn/hidden2/Relu\n",
      "hidden3/kernel/Initializer/random_uniform/shape\n",
      "hidden3/kernel/Initializer/random_uniform/min\n",
      "hidden3/kernel/Initializer/random_uniform/max\n",
      "hidden3/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden3/kernel/Initializer/random_uniform/sub\n",
      "hidden3/kernel/Initializer/random_uniform/mul\n",
      "hidden3/kernel/Initializer/random_uniform\n",
      "hidden3/kernel\n",
      "hidden3/kernel/Assign\n",
      "hidden3/kernel/read\n",
      "hidden3/bias/Initializer/zeros\n",
      "hidden3/bias\n",
      "hidden3/bias/Assign\n",
      "hidden3/bias/read\n",
      "dnn/hidden3/MatMul\n",
      "dnn/hidden3/BiasAdd\n",
      "dnn/hidden3/Relu\n",
      "hidden4/kernel/Initializer/random_uniform/shape\n",
      "hidden4/kernel/Initializer/random_uniform/min\n",
      "hidden4/kernel/Initializer/random_uniform/max\n",
      "hidden4/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden4/kernel/Initializer/random_uniform/sub\n",
      "hidden4/kernel/Initializer/random_uniform/mul\n",
      "hidden4/kernel/Initializer/random_uniform\n",
      "hidden4/kernel\n",
      "hidden4/kernel/Assign\n",
      "hidden4/kernel/read\n",
      "hidden4/bias/Initializer/zeros\n",
      "hidden4/bias\n",
      "hidden4/bias/Assign\n",
      "hidden4/bias/read\n",
      "dnn/hidden4/MatMul\n",
      "dnn/hidden4/BiasAdd\n",
      "dnn/hidden4/Relu\n",
      "hidden5/kernel/Initializer/random_uniform/shape\n",
      "hidden5/kernel/Initializer/random_uniform/min\n",
      "hidden5/kernel/Initializer/random_uniform/max\n",
      "hidden5/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden5/kernel/Initializer/random_uniform/sub\n",
      "hidden5/kernel/Initializer/random_uniform/mul\n",
      "hidden5/kernel/Initializer/random_uniform\n",
      "hidden5/kernel\n",
      "hidden5/kernel/Assign\n",
      "hidden5/kernel/read\n",
      "hidden5/bias/Initializer/zeros\n",
      "hidden5/bias\n",
      "hidden5/bias/Assign\n",
      "hidden5/bias/read\n",
      "dnn/hidden5/MatMul\n",
      "dnn/hidden5/BiasAdd\n",
      "dnn/hidden5/Relu\n",
      "outputs/kernel/Initializer/random_uniform/shape\n",
      "outputs/kernel/Initializer/random_uniform/min\n",
      "outputs/kernel/Initializer/random_uniform/max\n",
      "outputs/kernel/Initializer/random_uniform/RandomUniform\n",
      "outputs/kernel/Initializer/random_uniform/sub\n",
      "outputs/kernel/Initializer/random_uniform/mul\n",
      "outputs/kernel/Initializer/random_uniform\n",
      "outputs/kernel\n",
      "outputs/kernel/Assign\n",
      "outputs/kernel/read\n",
      "outputs/bias/Initializer/zeros\n",
      "outputs/bias\n",
      "outputs/bias/Assign\n",
      "outputs/bias/read\n",
      "dnn/outputs/MatMul\n",
      "dnn/outputs/BiasAdd\n",
      "loss/SparseSoftmaxCrossEntropyWithLogits/Shape\n",
      "loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits\n",
      "loss/Const\n",
      "loss/loss\n",
      "gradients/Shape\n",
      "gradients/grad_ys_0\n",
      "gradients/Fill\n",
      "gradients/loss/loss_grad/Reshape/shape\n",
      "gradients/loss/loss_grad/Reshape\n",
      "gradients/loss/loss_grad/Shape\n",
      "gradients/loss/loss_grad/Tile\n",
      "gradients/loss/loss_grad/Shape_1\n",
      "gradients/loss/loss_grad/Shape_2\n",
      "gradients/loss/loss_grad/Const\n",
      "gradients/loss/loss_grad/Prod\n",
      "gradients/loss/loss_grad/Const_1\n",
      "gradients/loss/loss_grad/Prod_1\n",
      "gradients/loss/loss_grad/Maximum/y\n",
      "gradients/loss/loss_grad/Maximum\n",
      "gradients/loss/loss_grad/floordiv\n",
      "gradients/loss/loss_grad/Cast\n",
      "gradients/loss/loss_grad/truediv\n",
      "gradients/zeros_like\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul\n",
      "gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/outputs/MatMul_grad/MatMul\n",
      "gradients/dnn/outputs/MatMul_grad/MatMul_1\n",
      "gradients/dnn/outputs/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/outputs/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden5/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden5/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden5/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden5/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden4/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden4/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden4/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden4/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden3/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden3/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden3/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden3/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden2/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden2/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden2/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden2/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden1/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden1/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden1/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden1/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency_1\n",
      "clip_by_value/Minimum/y\n",
      "clip_by_value/Minimum\n",
      "clip_by_value/y\n",
      "clip_by_value\n",
      "clip_by_value_1/Minimum/y\n",
      "clip_by_value_1/Minimum\n",
      "clip_by_value_1/y\n",
      "clip_by_value_1\n",
      "clip_by_value_2/Minimum/y\n",
      "clip_by_value_2/Minimum\n",
      "clip_by_value_2/y\n",
      "clip_by_value_2\n",
      "clip_by_value_3/Minimum/y\n",
      "clip_by_value_3/Minimum\n",
      "clip_by_value_3/y\n",
      "clip_by_value_3\n",
      "clip_by_value_4/Minimum/y\n",
      "clip_by_value_4/Minimum\n",
      "clip_by_value_4/y\n",
      "clip_by_value_4\n",
      "clip_by_value_5/Minimum/y\n",
      "clip_by_value_5/Minimum\n",
      "clip_by_value_5/y\n",
      "clip_by_value_5\n",
      "clip_by_value_6/Minimum/y\n",
      "clip_by_value_6/Minimum\n",
      "clip_by_value_6/y\n",
      "clip_by_value_6\n",
      "clip_by_value_7/Minimum/y\n",
      "clip_by_value_7/Minimum\n",
      "clip_by_value_7/y\n",
      "clip_by_value_7\n",
      "clip_by_value_8/Minimum/y\n",
      "clip_by_value_8/Minimum\n",
      "clip_by_value_8/y\n",
      "clip_by_value_8\n",
      "clip_by_value_9/Minimum/y\n",
      "clip_by_value_9/Minimum\n",
      "clip_by_value_9/y\n",
      "clip_by_value_9\n",
      "clip_by_value_10/Minimum/y\n",
      "clip_by_value_10/Minimum\n",
      "clip_by_value_10/y\n",
      "clip_by_value_10\n",
      "clip_by_value_11/Minimum/y\n",
      "clip_by_value_11/Minimum\n",
      "clip_by_value_11/y\n",
      "clip_by_value_11\n",
      "GradientDescent/learning_rate\n",
      "GradientDescent/update_hidden1/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden1/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden2/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden2/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden3/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden3/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden4/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden4/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden5/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden5/bias/ApplyGradientDescent\n",
      "GradientDescent/update_outputs/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_outputs/bias/ApplyGradientDescent\n",
      "GradientDescent\n",
      "eval/in_top_k/InTopKV2/k\n",
      "eval/in_top_k/InTopKV2\n",
      "eval/Cast\n",
      "eval/Const\n",
      "eval/accuracy\n",
      "init\n",
      "save/filename/input\n",
      "save/filename\n",
      "save/Const\n",
      "save/SaveV2/tensor_names\n",
      "save/SaveV2/shape_and_slices\n",
      "save/SaveV2\n",
      "save/control_dependency\n",
      "save/RestoreV2/tensor_names\n",
      "save/RestoreV2/shape_and_slices\n",
      "save/RestoreV2\n",
      "save/Assign\n",
      "save/Assign_1\n",
      "save/Assign_2\n",
      "save/Assign_3\n",
      "save/Assign_4\n",
      "save/Assign_5\n",
      "save/Assign_6\n",
      "save/Assign_7\n",
      "save/Assign_8\n",
      "save/Assign_9\n",
      "save/Assign_10\n",
      "save/Assign_11\n",
      "save/restore_all\n"
     ]
    }
   ],
   "source": [
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como son muchas operacioens es más facil utilizar Tensorboard para visualizar la gráfica.  Nuevamente utilizaremos la función gráfica implementada en Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = tf.compat.as_bytes(\"<stripped %d bytes>\"%size)\n",
    "    return strip_def\n",
    "  \n",
    "def rename_nodes(graph_def, rename_func):\n",
    "    res_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = res_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        n.name = rename_func(n.name)\n",
    "        for i, s in enumerate(n.input):\n",
    "            n.input[i] = rename_func(s) if s[0]!='^' else '^'+rename_func(s[1:])\n",
    "    return res_def\n",
    "  \n",
    "def show_graph(graph_def, max_const_size=32, width=800, height=600):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:{height}px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()), height=str(height))\n",
    "  \n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:{width}px;height:{height}px;border:0\" srcdoc=\"{code}\"></iframe>\n",
    "    \"\"\".format(code=code.replace('\"', '&quot;'), width=width, height=str(height + 20))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:800px;height:620px;border:0\" srcdoc=\"\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.3745401188473625&quot;).pbtxt = 'node {\\n  name: &quot;X&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: -1\\n        }\\n        dim {\\n          size: 784\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;y&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        unknown_rank: true\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\020\\\\003\\\\000\\\\000,\\\\001\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.07439795136451721\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.07439795136451721\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 5\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 784\\n        }\\n        dim {\\n          size: 300\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 300\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 300\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden1/bias&quot;\\n  input: &quot;hidden1/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden1/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden1/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;X&quot;\\n  input: &quot;hidden1/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden1/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/hidden1/MatMul&quot;\\n  input: &quot;hidden1/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden1/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;dnn/hidden1/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;,\\\\001\\\\000\\\\0002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.13093073666095734\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.13093073666095734\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 22\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 300\\n        }\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 50\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden2/bias&quot;\\n  input: &quot;hidden2/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden2/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden2/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden1/Relu&quot;\\n  input: &quot;hidden2/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden2/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/hidden2/MatMul&quot;\\n  input: &quot;hidden2/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden2/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;dnn/hidden2/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;2\\\\000\\\\000\\\\0002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 39\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden3/kernel&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden3/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 50\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden3/bias&quot;\\n  input: &quot;hidden3/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden3/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden3/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden2/Relu&quot;\\n  input: &quot;hidden3/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden3/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/hidden3/MatMul&quot;\\n  input: &quot;hidden3/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden3/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;dnn/hidden3/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;2\\\\000\\\\000\\\\0002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 56\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden4/kernel&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden4/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 50\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden4/bias&quot;\\n  input: &quot;hidden4/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden4/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden4/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden3/Relu&quot;\\n  input: &quot;hidden4/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden4/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/hidden4/MatMul&quot;\\n  input: &quot;hidden4/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden4/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;dnn/hidden4/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;2\\\\000\\\\000\\\\0002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 73\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden5/kernel&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden5/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 50\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden5/bias&quot;\\n  input: &quot;hidden5/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden5/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden5/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden4/Relu&quot;\\n  input: &quot;hidden5/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden5/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/hidden5/MatMul&quot;\\n  input: &quot;hidden5/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden5/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;dnn/hidden5/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;2\\\\000\\\\000\\\\000\\\\n\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.3162277638912201\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.3162277638912201\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 90\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n        dim {\\n          size: 10\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;outputs/kernel&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;outputs/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 10\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 10\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;outputs/bias&quot;\\n  input: &quot;outputs/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;outputs/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/outputs/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden5/Relu&quot;\\n  input: &quot;outputs/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/outputs/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/outputs/MatMul&quot;\\n  input: &quot;outputs/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  op: &quot;SparseSoftmaxCrossEntropyWithLogits&quot;\\n  input: &quot;dnn/outputs/BiasAdd&quot;\\n  input: &quot;y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tlabels&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/loss&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  input: &quot;loss/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/grad_ys_0&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Shape&quot;\\n  input: &quot;gradients/grad_ys_0&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;index_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Fill&quot;\\n  input: &quot;gradients/loss/loss_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/loss/loss_grad/Reshape&quot;\\n  input: &quot;gradients/loss/loss_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Shape_2&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Prod&quot;\\n  op: &quot;Prod&quot;\\n  input: &quot;gradients/loss/loss_grad/Shape_1&quot;\\n  input: &quot;gradients/loss/loss_grad/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Const_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Prod_1&quot;\\n  op: &quot;Prod&quot;\\n  input: &quot;gradients/loss/loss_grad/Shape_2&quot;\\n  input: &quot;gradients/loss/loss_grad/Const_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Maximum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;gradients/loss/loss_grad/Prod_1&quot;\\n  input: &quot;gradients/loss/loss_grad/Maximum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/floordiv&quot;\\n  op: &quot;FloorDiv&quot;\\n  input: &quot;gradients/loss/loss_grad/Prod&quot;\\n  input: &quot;gradients/loss/loss_grad/Maximum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Cast&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;gradients/loss/loss_grad/floordiv&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Truncate&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/truediv&quot;\\n  op: &quot;RealDiv&quot;\\n  input: &quot;gradients/loss/loss_grad/Tile&quot;\\n  input: &quot;gradients/loss/loss_grad/Cast&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/zeros_like&quot;\\n  op: &quot;ZerosLike&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient&quot;\\n  op: &quot;PreventGradient&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;message&quot;\\n    value {\\n      s: &quot;Currently there is no way to take the second derivative of sparse_softmax_cross_entropy_with_logits due to the fused implementation\\\\\\'s interaction with tf.gradients()&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: -1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims&quot;\\n  op: &quot;ExpandDims&quot;\\n  input: &quot;gradients/loss/loss_grad/truediv&quot;\\n  input: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tdim&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims&quot;\\n  input: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n  input: &quot;^gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;outputs/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden5/Relu&quot;\\n  input: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/outputs/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/outputs/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/outputs/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/outputs/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/outputs/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/outputs/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/dnn/outputs/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/outputs/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/dnn/outputs/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden5/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/dnn/hidden5/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden5/Relu_grad/ReluGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden5/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden5/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden5/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden4/Relu&quot;\\n  input: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden5/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden5/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden5/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden5/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden5/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden5/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/dnn/hidden5/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden5/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden4/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden4/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden3/Relu&quot;\\n  input: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden4/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden4/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden4/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden4/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden4/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden4/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/dnn/hidden4/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden4/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden3/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden3/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden2/Relu&quot;\\n  input: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden3/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden3/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden3/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden3/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden3/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden3/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/dnn/hidden3/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden3/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden2/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/dnn/hidden2/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden2/Relu_grad/ReluGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden2/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden2/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden2/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden1/Relu&quot;\\n  input: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden2/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden2/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden2/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden2/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden2/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden2/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/dnn/hidden2/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden2/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden1/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/dnn/hidden1/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden1/Relu_grad/ReluGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden1/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden1/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden1/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;X&quot;\\n  input: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden1/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden1/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden1/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden1/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden1/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden1/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/dnn/hidden1/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden1/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value/Minimum&quot;\\n  input: &quot;clip_by_value/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_1/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_1/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_1/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_1/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_1&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_1/Minimum&quot;\\n  input: &quot;clip_by_value_1/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_2/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_2/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_2/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_2/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_2&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_2/Minimum&quot;\\n  input: &quot;clip_by_value_2/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_3/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_3/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_3/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_3/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_3&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_3/Minimum&quot;\\n  input: &quot;clip_by_value_3/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_4/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_4/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_4/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_4/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_4&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_4/Minimum&quot;\\n  input: &quot;clip_by_value_4/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_5/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_5/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_5/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_5/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_5&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_5/Minimum&quot;\\n  input: &quot;clip_by_value_5/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_6/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_6/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_6/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_6/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_6&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_6/Minimum&quot;\\n  input: &quot;clip_by_value_6/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_7/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_7/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_7/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_7/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_7&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_7/Minimum&quot;\\n  input: &quot;clip_by_value_7/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_8/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_8/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_8/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_8/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_8&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_8/Minimum&quot;\\n  input: &quot;clip_by_value_8/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_9/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_9/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_9/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_9/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_9&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_9/Minimum&quot;\\n  input: &quot;clip_by_value_9/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_10/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_10/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_10/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_10/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_10&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_10/Minimum&quot;\\n  input: &quot;clip_by_value_10/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_11/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_11/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_11/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_11/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_11&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_11/Minimum&quot;\\n  input: &quot;clip_by_value_11/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/learning_rate&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.009999999776482582\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden1/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden1/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden1/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden2/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden2/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden2/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden3/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden3/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_4&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden3/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden3/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_5&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden4/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden4/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_6&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden4/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden4/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_7&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden5/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden5/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_8&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden5/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden5/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_9&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_outputs/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;outputs/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_10&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_outputs/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;outputs/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_11&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^GradientDescent/update_hidden1/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden1/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden2/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden2/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden3/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden3/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden4/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden4/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden5/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden5/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_outputs/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_outputs/kernel/ApplyGradientDescent&quot;\\n}\\nnode {\\n  name: &quot;eval/in_top_k/InTopKV2/k&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;eval/in_top_k/InTopKV2&quot;\\n  op: &quot;InTopKV2&quot;\\n  input: &quot;dnn/outputs/BiasAdd&quot;\\n  input: &quot;y&quot;\\n  input: &quot;eval/in_top_k/InTopKV2/k&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;eval/Cast&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;eval/in_top_k/InTopKV2&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_BOOL\\n    }\\n  }\\n  attr {\\n    key: &quot;Truncate&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;eval/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;eval/accuracy&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;eval/Cast&quot;\\n  input: &quot;eval/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;init&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^hidden1/bias/Assign&quot;\\n  input: &quot;^hidden1/kernel/Assign&quot;\\n  input: &quot;^hidden2/bias/Assign&quot;\\n  input: &quot;^hidden2/kernel/Assign&quot;\\n  input: &quot;^hidden3/bias/Assign&quot;\\n  input: &quot;^hidden3/kernel/Assign&quot;\\n  input: &quot;^hidden4/bias/Assign&quot;\\n  input: &quot;^hidden4/kernel/Assign&quot;\\n  input: &quot;^hidden5/bias/Assign&quot;\\n  input: &quot;^hidden5/kernel/Assign&quot;\\n  input: &quot;^outputs/bias/Assign&quot;\\n  input: &quot;^outputs/kernel/Assign&quot;\\n}\\nnode {\\n  name: &quot;save/filename/input&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n        }\\n        string_val: &quot;model&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/filename&quot;\\n  op: &quot;PlaceholderWithDefault&quot;\\n  input: &quot;save/filename/input&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Const&quot;\\n  op: &quot;PlaceholderWithDefault&quot;\\n  input: &quot;save/filename&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 12\\n          }\\n        }\\n        string_val: &quot;hidden1/bias&quot;\\n        string_val: &quot;hidden1/kernel&quot;\\n        string_val: &quot;hidden2/bias&quot;\\n        string_val: &quot;hidden2/kernel&quot;\\n        string_val: &quot;hidden3/bias&quot;\\n        string_val: &quot;hidden3/kernel&quot;\\n        string_val: &quot;hidden4/bias&quot;\\n        string_val: &quot;hidden4/kernel&quot;\\n        string_val: &quot;hidden5/bias&quot;\\n        string_val: &quot;hidden5/kernel&quot;\\n        string_val: &quot;outputs/bias&quot;\\n        string_val: &quot;outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 12\\n          }\\n        }\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2&quot;\\n  op: &quot;SaveV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/SaveV2/tensor_names&quot;\\n  input: &quot;save/SaveV2/shape_and_slices&quot;\\n  input: &quot;hidden1/bias&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  input: &quot;hidden2/bias&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  input: &quot;hidden3/bias&quot;\\n  input: &quot;hidden3/kernel&quot;\\n  input: &quot;hidden4/bias&quot;\\n  input: &quot;hidden4/kernel&quot;\\n  input: &quot;hidden5/bias&quot;\\n  input: &quot;hidden5/kernel&quot;\\n  input: &quot;outputs/bias&quot;\\n  input: &quot;outputs/kernel&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;^save/SaveV2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@save/Const&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 12\\n          }\\n        }\\n        string_val: &quot;hidden1/bias&quot;\\n        string_val: &quot;hidden1/kernel&quot;\\n        string_val: &quot;hidden2/bias&quot;\\n        string_val: &quot;hidden2/kernel&quot;\\n        string_val: &quot;hidden3/bias&quot;\\n        string_val: &quot;hidden3/kernel&quot;\\n        string_val: &quot;hidden4/bias&quot;\\n        string_val: &quot;hidden4/kernel&quot;\\n        string_val: &quot;hidden5/bias&quot;\\n        string_val: &quot;hidden5/kernel&quot;\\n        string_val: &quot;outputs/bias&quot;\\n        string_val: &quot;outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 12\\n          }\\n        }\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2/tensor_names&quot;\\n  input: &quot;save/RestoreV2/shape_and_slices&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden1/bias&quot;\\n  input: &quot;save/RestoreV2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_1&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  input: &quot;save/RestoreV2:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_2&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden2/bias&quot;\\n  input: &quot;save/RestoreV2:2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_3&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  input: &quot;save/RestoreV2:3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_4&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden3/bias&quot;\\n  input: &quot;save/RestoreV2:4&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_5&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden3/kernel&quot;\\n  input: &quot;save/RestoreV2:5&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_6&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden4/bias&quot;\\n  input: &quot;save/RestoreV2:6&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_7&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden4/kernel&quot;\\n  input: &quot;save/RestoreV2:7&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_8&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden5/bias&quot;\\n  input: &quot;save/RestoreV2:8&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_9&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden5/kernel&quot;\\n  input: &quot;save/RestoreV2:9&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_10&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;outputs/bias&quot;\\n  input: &quot;save/RestoreV2:10&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_11&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;outputs/kernel&quot;\\n  input: &quot;save/RestoreV2:11&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/restore_all&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^save/Assign&quot;\\n  input: &quot;^save/Assign_1&quot;\\n  input: &quot;^save/Assign_10&quot;\\n  input: &quot;^save/Assign_11&quot;\\n  input: &quot;^save/Assign_2&quot;\\n  input: &quot;^save/Assign_3&quot;\\n  input: &quot;^save/Assign_4&quot;\\n  input: &quot;^save/Assign_5&quot;\\n  input: &quot;^save/Assign_6&quot;\\n  input: &quot;^save/Assign_7&quot;\\n  input: &quot;^save/Assign_8&quot;\\n  input: &quot;^save/Assign_9&quot;\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.3745401188473625&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como ahora se visualiza las operaciones necesarias, puede utilizar las debidas funciones `get_operation_by_name()` o `get_tensor_by_name()` para adquirir las entradas, función de perdida y las ultimas capas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name(\"eval/accuracy:0\")\n",
    "\n",
    "training_op = tf.get_default_graph().get_operation_by_name(\"GradientDescent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el codigo siguiente podemos compactar el uso de las variables para legibilidad, de esta manera se tienen funciones bien documentadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "for op in (X, y, accuracy, training_op):\n",
    "    tf.add_to_collection(\"my_important_ops\", op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y para reutilizar el modelo simplemente se tiene:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, accuracy, training_op = tf.get_collection(\"my_important_ops\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se inicia la sesión, restaura el modelo y se continua con el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "    # continue training the model..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos nuevamente el training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.9634\n",
      "1 Validation accuracy: 0.9632\n",
      "2 Validation accuracy: 0.9656\n",
      "3 Validation accuracy: 0.9652\n",
      "4 Validation accuracy: 0.9642\n",
      "5 Validation accuracy: 0.9648\n",
      "6 Validation accuracy: 0.9686\n",
      "7 Validation accuracy: 0.9684\n",
      "8 Validation accuracy: 0.9686\n",
      "9 Validation accuracy: 0.9686\n",
      "10 Validation accuracy: 0.9704\n",
      "11 Validation accuracy: 0.9718\n",
      "12 Validation accuracy: 0.9674\n",
      "13 Validation accuracy: 0.97\n",
      "14 Validation accuracy: 0.9708\n",
      "15 Validation accuracy: 0.9722\n",
      "16 Validation accuracy: 0.972\n",
      "17 Validation accuracy: 0.971\n",
      "18 Validation accuracy: 0.9712\n",
      "19 Validation accuracy: 0.971\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si utilizamos `import_meta_graph()` es porque tenemos acceso a la gráfica.  Pero normalmente se requiere para transfer learning es tener las capas inferiores y solamente modificar las ultimas capas o construir nuevas capas, crear funciones de optimización, una nueva salida, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 50\n",
    "n_hidden5 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"hidden5\")\n",
    "    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "threshold = 1.0\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
    "              for grad, var in grads_and_vars]\n",
    "training_op = optimizer.apply_gradients(capped_gvs)\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y continuamos el entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.9644\n",
      "1 Validation accuracy: 0.9632\n",
      "2 Validation accuracy: 0.9654\n",
      "3 Validation accuracy: 0.9656\n",
      "4 Validation accuracy: 0.9644\n",
      "5 Validation accuracy: 0.965\n",
      "6 Validation accuracy: 0.9686\n",
      "7 Validation accuracy: 0.9686\n",
      "8 Validation accuracy: 0.9686\n",
      "9 Validation accuracy: 0.9684\n",
      "10 Validation accuracy: 0.9702\n",
      "11 Validation accuracy: 0.9716\n",
      "12 Validation accuracy: 0.9672\n",
      "13 Validation accuracy: 0.97\n",
      "14 Validation accuracy: 0.9706\n",
      "15 Validation accuracy: 0.9722\n",
      "16 Validation accuracy: 0.9716\n",
      "17 Validation accuracy: 0.971\n",
      "18 Validation accuracy: 0.9712\n",
      "19 Validation accuracy: 0.9716\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general, buscamos utilizar las capas que queremos reentrenar.  Si se utiliza`import_meta_graph()` cargará toda la gráfica, uno simplemente debe ignorar las partes que no se necesita.  Por ejemplo añadimos en el código inferior una cuarta capa oculta al borde de la tercera capa pre entrenada (ignorando la 4ta capa).  Construimos una nueva capa de salida y una función de pérdida además de un nuevo optimizador para minimizar.  También necesitamos otro 'saver' para la nueva gráfica (contiene la grafica vieja y las nuevas operaciones), y sus inicializaciones de las nuevas variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_hidden4 = 20  # new layer\n",
    "n_outputs = 10  # new layer\n",
    "\n",
    "saver = tf.train.import_meta_graph(\"./my_model_final.ckpt.meta\")\n",
    "\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "\n",
    "hidden3 = tf.get_default_graph().get_tensor_by_name(\"dnn/hidden3/Relu:0\")\n",
    "\n",
    "new_hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"new_hidden4\")\n",
    "new_logits = tf.layers.dense(new_hidden4, n_outputs, name=\"new_outputs\")\n",
    "\n",
    "with tf.name_scope(\"new_loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=new_logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"new_eval\"):\n",
    "    correct = tf.nn.in_top_k(new_logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"new_train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "new_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y entrenamos este nuevo modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.9186\n",
      "1 Validation accuracy: 0.9392\n",
      "2 Validation accuracy: 0.9486\n",
      "3 Validation accuracy: 0.9528\n",
      "4 Validation accuracy: 0.9554\n",
      "5 Validation accuracy: 0.9556\n",
      "6 Validation accuracy: 0.9574\n",
      "7 Validation accuracy: 0.9612\n",
      "8 Validation accuracy: 0.9612\n",
      "9 Validation accuracy: 0.9644\n",
      "10 Validation accuracy: 0.965\n",
      "11 Validation accuracy: 0.9658\n",
      "12 Validation accuracy: 0.964\n",
      "13 Validation accuracy: 0.9672\n",
      "14 Validation accuracy: 0.969\n",
      "15 Validation accuracy: 0.9682\n",
      "16 Validation accuracy: 0.9702\n",
      "17 Validation accuracy: 0.9678\n",
      "18 Validation accuracy: 0.9696\n",
      "19 Validation accuracy: 0.9698\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = new_saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si tiene acceso al código de python que construyó la gráfica original, puede reutilizar las partes que necesitan y retirar el resto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # reused\n",
    "n_hidden2 = 50  # reused\n",
    "n_hidden3 = 50  # reused\n",
    "n_hidden4 = 20  # new!\n",
    "n_outputs = 10  # new!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")       # reused\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\") # reused\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\") # reused\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\") # new!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\")                         # new!\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cabe destacar quie se necesita construir un 'Saver' para cargar y otro 'Saver' para guardar, de lo contrario las gráficas no coincidirán, el último es del modelo re entrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.9018\n",
      "1 Validation accuracy: 0.9336\n",
      "2 Validation accuracy: 0.9428\n",
      "3 Validation accuracy: 0.947\n",
      "4 Validation accuracy: 0.9518\n",
      "5 Validation accuracy: 0.953\n",
      "6 Validation accuracy: 0.9558\n",
      "7 Validation accuracy: 0.9594\n",
      "8 Validation accuracy: 0.9584\n",
      "9 Validation accuracy: 0.9606\n",
      "10 Validation accuracy: 0.9626\n",
      "11 Validation accuracy: 0.962\n",
      "12 Validation accuracy: 0.9636\n",
      "13 Validation accuracy: 0.966\n",
      "14 Validation accuracy: 0.966\n",
      "15 Validation accuracy: 0.966\n",
      "16 Validation accuracy: 0.9672\n",
      "17 Validation accuracy: 0.9676\n",
      "18 Validation accuracy: 0.9682\n",
      "19 Validation accuracy: 0.968\n"
     ]
    }
   ],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                               scope=\"hidden[123]\") # regular expression\n",
    "restore_saver = tf.train.Saver(reuse_vars) # to restore layers 1-3\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):                                            # not shown in the book\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size): # not shown\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})        # not shown\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})     # not shown\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)                   # not shown\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reutilizando Modelos de Otros 'Frameworks'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si no utiliza Tensorflow, entonces para cada variable a utilizar, si es otro framework com PyTorch o Theano se debe cargar las variables apropiadas, esto es un poco tedioso.  Debajo se muestra la manera.  Primero se ubica los operadores de inicialización y luego vemos la segunda entrada, la cual corresponde a valor de inicialización. Cuando se corre el inicializador luego reemplazamos los valores de inicialización con un `feed_dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 2\n",
    "n_hidden1 = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 61.  83. 105.]]\n"
     ]
    }
   ],
   "source": [
    "original_w = [[1., 2., 3.], [4., 5., 6.]] # Load the weights from the other framework\n",
    "original_b = [7., 8., 9.]                 # Load the biases from the other framework\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "# [...] Build the rest of the model\n",
    "\n",
    "# Get a handle on the assignment nodes for the hidden1 variables\n",
    "graph = tf.get_default_graph()\n",
    "assign_kernel = graph.get_operation_by_name(\"hidden1/kernel/Assign\")\n",
    "assign_bias = graph.get_operation_by_name(\"hidden1/bias/Assign\")\n",
    "init_kernel = assign_kernel.inputs[1]\n",
    "init_bias = assign_bias.inputs[1]\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init, feed_dict={init_kernel: original_w, init_bias: original_b})\n",
    "    # [...] Train the model on your new task\n",
    "    print(hidden1.eval(feed_dict={X: [[10.0, 11.0]]}))  # not shown in the book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra alternativa es crear una asignación dedicada a los nodos y placeholders.  Esta es tiene más ayuda en salida y es menos eficiente, pero es más explícita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 61.  83. 105.]]\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 2\n",
    "n_hidden1 = 3\n",
    "\n",
    "original_w = [[1., 2., 3.], [4., 5., 6.]] # Load the weights from the other framework\n",
    "original_b = [7., 8., 9.]                 # Load the biases from the other framework\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "# [...] Build the rest of the model\n",
    "\n",
    "# Get a handle on the variables of layer hidden1\n",
    "with tf.variable_scope(\"\", default_name=\"\", reuse=True):  # root scope\n",
    "    hidden1_weights = tf.get_variable(\"hidden1/kernel\")\n",
    "    hidden1_biases = tf.get_variable(\"hidden1/bias\")\n",
    "\n",
    "# Create dedicated placeholders and assignment nodes\n",
    "original_weights = tf.placeholder(tf.float32, shape=(n_inputs, n_hidden1))\n",
    "original_biases = tf.placeholder(tf.float32, shape=n_hidden1)\n",
    "assign_hidden1_weights = tf.assign(hidden1_weights, original_weights)\n",
    "assign_hidden1_biases = tf.assign(hidden1_biases, original_biases)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    sess.run(assign_hidden1_weights, feed_dict={original_weights: original_w})\n",
    "    sess.run(assign_hidden1_biases, feed_dict={original_biases: original_b})\n",
    "    # [...] Train the model on your new task\n",
    "    print(hidden1.eval(feed_dict={X: [[10.0, 11.0]]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También nótese que se puede tener la variable `get_collection()` y especificar `scope`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'hidden1/kernel:0' shape=(2, 3) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden1/bias:0' shape=(3,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"hidden1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O podemos utilizar el método de la gráfica `get_tensor_by_name()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'hidden1/kernel:0' shape=(2, 3) dtype=float32_ref>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'hidden1/bias:0' shape=(3,) dtype=float32_ref>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_default_graph().get_tensor_by_name(\"hidden1/bias:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congelando las capas inferiores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como las caps inferiores de la red neuronal detectan características de bajo nivel en las figuras, podemos reutilizar estas capas como son.  En general es una buena idea 'congelar' los pesos cuando se entrena una nueva red neuronal para que sea más fácil de entrenar esta última capa.\n",
    "\n",
    "La siguiente es la primera manera, es solamente dando el nombre de las variables a entrenar, detallaremos en la sección que se utiliza más adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # reused\n",
    "n_hidden2 = 50  # reused\n",
    "n_hidden3 = 50  # reused\n",
    "n_hidden4 = 20  # new!\n",
    "n_outputs = 10  # new!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")       # reused\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\") # reused\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\") # reused\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\") # new!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\")                         # new!\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):                                         # not shown in the book\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)     # not shown\n",
    "    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                                   scope=\"hidden[34]|outputs\")\n",
    "    training_op = optimizer.minimize(loss, var_list=train_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el código anterior utilizamos las variables de entrenamiento que se extraen de las capas 3 y 4 en la capa de salida.  Esto deja las variables en las capas ocultas 1 y 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.8954\n",
      "1 Validation accuracy: 0.93\n",
      "2 Validation accuracy: 0.9404\n",
      "3 Validation accuracy: 0.9444\n",
      "4 Validation accuracy: 0.9478\n",
      "5 Validation accuracy: 0.9508\n",
      "6 Validation accuracy: 0.9508\n",
      "7 Validation accuracy: 0.9534\n",
      "8 Validation accuracy: 0.9552\n",
      "9 Validation accuracy: 0.9564\n",
      "10 Validation accuracy: 0.956\n",
      "11 Validation accuracy: 0.957\n",
      "12 Validation accuracy: 0.9572\n",
      "13 Validation accuracy: 0.9578\n",
      "14 Validation accuracy: 0.959\n",
      "15 Validation accuracy: 0.9578\n",
      "16 Validation accuracy: 0.9576\n",
      "17 Validation accuracy: 0.9602\n",
      "18 Validation accuracy: 0.959\n",
      "19 Validation accuracy: 0.9602\n"
     ]
    }
   ],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                               scope=\"hidden[123]\") # regular expression\n",
    "restore_saver = tf.train.Saver(reuse_vars) # to restore layers 1-3\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # reused\n",
    "n_hidden2 = 50  # reused\n",
    "n_hidden3 = 50  # reused\n",
    "n_hidden4 = 20  # new!\n",
    "n_outputs = 10  # new!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                              name=\"hidden1\") # reused frozen\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n",
    "                              name=\"hidden2\") # reused frozen\n",
    "    hidden2_stop = tf.stop_gradient(hidden2)\n",
    "    hidden3 = tf.layers.dense(hidden2_stop, n_hidden3, activation=tf.nn.relu,\n",
    "                              name=\"hidden3\") # reused, not frozen\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu,\n",
    "                              name=\"hidden4\") # new!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\") # new!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Del código anterior podemos ver otra opción la cual es deteniendo o manteniendo los pesos en las capas 1 y 2 y reentrenando las capas 3 y 4 que son las finales.\n",
    "\n",
    "Las siguientes lineas son más de lo mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El código de entrenamiento es igual al anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.902\n",
      "1 Validation accuracy: 0.931\n",
      "2 Validation accuracy: 0.943\n",
      "3 Validation accuracy: 0.948\n",
      "4 Validation accuracy: 0.9514\n",
      "5 Validation accuracy: 0.9524\n",
      "6 Validation accuracy: 0.9522\n",
      "7 Validation accuracy: 0.9554\n",
      "8 Validation accuracy: 0.9552\n",
      "9 Validation accuracy: 0.9558\n",
      "10 Validation accuracy: 0.957\n",
      "11 Validation accuracy: 0.9554\n",
      "12 Validation accuracy: 0.957\n",
      "13 Validation accuracy: 0.9578\n",
      "14 Validation accuracy: 0.9582\n",
      "15 Validation accuracy: 0.957\n",
      "16 Validation accuracy: 0.9566\n",
      "17 Validation accuracy: 0.9576\n",
      "18 Validation accuracy: 0.9592\n",
      "19 Validation accuracy: 0.9578\n"
     ]
    }
   ],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                               scope=\"hidden[123]\") # regular expression\n",
    "restore_saver = tf.train.Saver(reuse_vars) # to restore layers 1-3\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Almacenamiento en Caché de Capas Congeladas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como las capas inferiores no cambiarán es posible almacenar las salidas de estas capas congeladas en cada instancia, de esta manera acelerará el training pues solamente se visitarán estas capas una vez cada epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # reused\n",
    "n_hidden2 = 50  # reused\n",
    "n_hidden3 = 50  # reused\n",
    "n_hidden4 = 20  # new!\n",
    "n_outputs = 10  # new!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                              name=\"hidden1\") # reused frozen\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n",
    "                              name=\"hidden2\") # reused frozen & cached\n",
    "    hidden2_stop = tf.stop_gradient(hidden2)\n",
    "    hidden3 = tf.layers.dense(hidden2_stop, n_hidden3, activation=tf.nn.relu,\n",
    "                              name=\"hidden3\") # reused, not frozen\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu,\n",
    "                              name=\"hidden4\") # new!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\") # new!\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                               scope=\"hidden[123]\") # regular expression\n",
    "restore_saver = tf.train.Saver(reuse_vars) # to restore layers 1-3\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.902\n",
      "1 Validation accuracy: 0.931\n",
      "2 Validation accuracy: 0.943\n",
      "3 Validation accuracy: 0.948\n",
      "4 Validation accuracy: 0.9514\n",
      "5 Validation accuracy: 0.9524\n",
      "6 Validation accuracy: 0.9522\n",
      "7 Validation accuracy: 0.9554\n",
      "8 Validation accuracy: 0.9552\n",
      "9 Validation accuracy: 0.9558\n",
      "10 Validation accuracy: 0.957\n",
      "11 Validation accuracy: 0.9554\n",
      "12 Validation accuracy: 0.957\n",
      "13 Validation accuracy: 0.9578\n",
      "14 Validation accuracy: 0.9582\n",
      "15 Validation accuracy: 0.957\n",
      "16 Validation accuracy: 0.9566\n",
      "17 Validation accuracy: 0.9576\n",
      "18 Validation accuracy: 0.9592\n",
      "19 Validation accuracy: 0.9578\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_batches = len(X_train) // batch_size\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "    \n",
    "    h2_cache = sess.run(hidden2, feed_dict={X: X_train})\n",
    "    h2_cache_valid = sess.run(hidden2, feed_dict={X: X_valid}) # not shown in the book\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        shuffled_idx = np.random.permutation(len(X_train))\n",
    "        hidden2_batches = np.array_split(h2_cache[shuffled_idx], n_batches)\n",
    "        y_batches = np.array_split(y_train[shuffled_idx], n_batches)\n",
    "        for hidden2_batch, y_batch in zip(hidden2_batches, y_batches):\n",
    "            sess.run(training_op, feed_dict={hidden2:hidden2_batch, y:y_batch})\n",
    "\n",
    "        accuracy_val = accuracy.eval(feed_dict={hidden2: h2_cache_valid, # not shown\n",
    "                                                y: y_valid})             # not shown\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)               # not shown\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mejores Optimizadores (más rápidos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenar una red neuronal puede ser muy extenso y cansa, consume tiempo.  Hasta el momento se habló de como acelerar el entranamiento llegando a mejores soluciones como inicializar los pesos y buenas funcioens de activación, usar batch normalization y partes de una red neuronal para pre entrenar.  Existen otros optimizadores que se usan para acelerar el training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimización por Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es como tener un balón y dejarlo caer en una pendiente, inicialmente irá lento pero luego tomará velocidad gradual.  Esta técnica la propuso Boris Polyak en 1964 a diferencia de gradiente descendiente que toma saltos regulares hasta llegar hasta el fondo.\n",
    "\n",
    "Recuerde que el gradiente actualiza los pesos de la función de costo $J(\\theta)$ utilizando también el $\\eta$.  [Momentum](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Momentum) opera por medio de donde quedaron los gradientes, y actualiza los pesos añadiendo el vecto de momentum. Además del Learning Rate que está en la ecuación también posee un parametro alfa (a veces también denotado por beta).\n",
    "\n",
    "El gradiente permaneces constante en la velocidad terminal (tamaño máximo de las actualizacioens de pesos).  Regla general es utilizar un $\\eta$ un valor dependiente de $\\beta$, así $\\frac{1}{1-\\beta}$.  Este correrá más rápido que el gradiente descendiente y también escapará de plateaux y de la óptima local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,\n",
    "                                       momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una cosa negativa de momentum es que utiliza un nuevo hiperparámetro, pero un valor de 0.9 es recomendado con buenos resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradiente Acelerado de Nesterov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yuri Nesterov en 1983 propone algo un poco mejor que la implementación solitaria de momentum que era medir el gradiente un poco despues de él, es decir es medido en $\\theta + \\beta m$ en vez de $\\theta$.\n",
    "\n",
    "Y funciona bien, es mejor utilizar esta medida del gradiente en su posición siguiente, siempre termina cercano al óptimo local permitioendo a las menores osicilaciones del algoritmo y converger rápidamente.\n",
    "\n",
    "El NAG (Nesterov Accelerated Gradient) se puede utilizar como sigue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,\n",
    "                                       momentum=0.9, use_nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora repasaremos lo que hace este optimizador.  A diferencia de gradiente descendiente que baja la pendiente más empinada este algoritmo detecta y escala el vector de gradiente a traves de las dimensiones más empinadas. [Adagrad](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#AdaGrad) acumula el cuadrado de los gradientes en un vector $s$ acumuklando la derivada parcial de la función de costo $\\theta_i$, esta se volverá más extensa en cada iteración hasta llegar al mínimo global.\n",
    "\n",
    "La siguiente parte del algoritmo se parece a gradiente descendiente pero el vector se reduce a un factor de $\\sqrt{s + \\epsilon}$, donde $\\epsilon$ es un valor bajo cercano a cero (10^10 p.e.).  En resumen hace decaer el learning rate pero más rápido y en pendientes más finas.  También llamado 'adaptative learning rate' y requiere menos ajuste porque solo es el parámetro $\\eta$.\n",
    "\n",
    "A pesar que lo hace relativamente bien, tiene problemas que se detiene tempranamente con redes neuronales debido a que escala mucho antes de llegar al minimo global óptimo.  EVITAR USAR ADAGRAD CON REDES NEURONALES!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaGrad llega al óptimo global como se dijo, pero en raras ocasiones, Este algoritmo fija esto acumulando solamente gradientes de las iteracioens recientes (opuesto a todos los gradientes desde el inicio de entrenamiento), simplemente utiliza una decaída exponencial.\n",
    "\n",
    "El hiperparámetro $\\beta$ normalmente con 0.89 da buenos resultados.  Preferiblemente utilizar este en ves de AdaGrad.\n",
    "\n",
    "El algoritmo se puede encontrar en [esta dirección](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#RMSProp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate,\n",
    "                                      momentum=0.9, decay=0.9, epsilon=1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Este](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam) algoritmo de optimización combina las ideas de momentum y RMSProp manteniendo los gradientes pasados exponencialmente decayendo y el decaimiento al cuadrado de los gradientes pasados.\n",
    "\n",
    "Favor compare las ecuaciones de RMSProp y Momentum para que note su similaridad.\n",
    "\n",
    "La desventaja es que se añaden dos hiperparámetros $\\beta_1$ y $\\beta_2$ pero que inicializados a 0.9 y 0.999 dan buenos resultados.  De esta manera su inicialización se puede hacer sencilla para $\\eta$ 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las optimizaciones anteriores confian en las derivadas parciales de primer orden (llamados también Jacobianos).  Las derivadas parciales de segundo orden o Hesianos son algoritmos muy óptimos sin embargo son difíciles de aplicar para redes neuronales porque hay $n^2$ Hesianos por cada salida.  Son difíciles de computar y no caben en la memoria de las computadoras actuales, además de ser lentos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agendamiento del Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es difícil encontrar un buen learning rate.  Si es muy alto puede diverger, si es miy bajo demorará mucho el entrenamiento, pero encontrará el óptimo local o el global.  Si se mantiene ligeramente alto irá descendiendo pero saltando sobre el óptimo global (fue para esto que recomendamos utilizar algoritmos adaptativos como RMSProp o Adam).\n",
    "\n",
    "La otra opción es interrumpir el entrenamiento que viaje a una solución subóptima.\n",
    "\n",
    "Puede encontrar un learning rate durante algunos epochs, pero el ideal será el que converja a la solución más rápidamente.\n",
    "\n",
    "Si pudiesemos iniciar con un LR alto y luego disminuirlo iríamos más rápido.  Ls estrategias más comunes son:\n",
    "- Un learning rate dividido en pedazos:  Es decir, por ejemplo, primero en 0.1 luego en 0.001 despues de 50 epochs, funciona bien, pero tendríamos que saber en donde ir dividiendo.\n",
    "- Scheduling de desempeño: midiendo el error cada N pasos (como en early stoping) y reducir el LR en un factor lambda\n",
    "- Scheduling exponencial: simplemente cambiar la función del LR a $\\eta(t) = \\eta_0 * 10^{t/r}$.  Funciona bien pero requiere de estas dos variables $\\eta_0$ y $r$.\n",
    "- Scheduling de Potencias:  Ajustar el learning rate a un valor exponencial en la función $\\eta(t) = \\eta_0 (1+t/r)^{-c}$. C es tipicamente ajustado a 1 para que el LR baje finamente.\n",
    "\n",
    "En el 2013 un paper de Andrew Senior evaluó todos los métodos y dedujo que el exponencial y de scheduling teian buen desempeño pero favorecia al exponencial que era más facil de implementar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):       # not shown in the book\n",
    "    initial_learning_rate = 0.1\n",
    "    decay_steps = 10000\n",
    "    decay_rate = 1/10\n",
    "    global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step,\n",
    "                                               decay_steps, decay_rate)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Del codigo anetrior vemos que TF tiene como realizar este con .exponential_decay,  luego creamos un optimizador de momentum sobre este LR, el algoritmo optimizará para $\\eta = 0.1$ y $r = 10000$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.9614\n",
      "1 Validation accuracy: 0.9712\n",
      "2 Validation accuracy: 0.9786\n",
      "3 Validation accuracy: 0.9794\n",
      "4 Validation accuracy: 0.9822\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previniendo Overfitting a Través de Regularización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalmente hemos visto que estas redes neuronales tienen muchos hiperparámetros qeu ajustar, a veces millones.  Esto a veces tiende a que el dataset y su entrenamiento recaigan en overfitting.\n",
    "\n",
    "Veremos algunos de los principales métodos para reducir overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parada Temprana\n",
    "\n",
    "Una solución sencilal es interrumpir el entrenamiento antes de que el validation set empiece a caer, luego se salva el modelo en ese punto y se detiene el entrenamiento.  Generalmente es una de las primeras cosas que se realiza pero también se puede realizar cosas mejores con otras técnicas de regularización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularización $\\ell_1$ y $\\ell_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplemente en tensorflow se debe añadir el termin de regularización en la función de costo, para cada caso de los pesos se debería implementar una regularización de la capa de pesos W1, W2 hsta Wn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    logits = tf.layers.dense(hidden1, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we get a handle on the layer weights, and we compute the total loss, which is equal to the sum of the usual cross entropy loss and the $\\ell_1$ loss (i.e., the absolute values of the weights):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")\n",
    "W2 = tf.get_default_graph().get_tensor_by_name(\"outputs/kernel:0\")\n",
    "\n",
    "scale = 0.001 # l1 regularization hyperparameter\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                              logits=logits)\n",
    "    base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")\n",
    "    reg_losses = tf.reduce_sum(tf.abs(W1)) + tf.reduce_sum(tf.abs(W2))\n",
    "    loss = tf.add(base_loss, scale * reg_losses, name=\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El código anterior implementa a mano la regularización solamente para una red neuronal de dos etapas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo siguiente es lo de siempre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.831\n",
      "1 Validation accuracy: 0.871\n",
      "2 Validation accuracy: 0.8838\n",
      "3 Validation accuracy: 0.8934\n",
      "4 Validation accuracy: 0.8966\n",
      "5 Validation accuracy: 0.8988\n",
      "6 Validation accuracy: 0.9016\n",
      "7 Validation accuracy: 0.9044\n",
      "8 Validation accuracy: 0.9058\n",
      "9 Validation accuracy: 0.906\n",
      "10 Validation accuracy: 0.9068\n",
      "11 Validation accuracy: 0.9054\n",
      "12 Validation accuracy: 0.907\n",
      "13 Validation accuracy: 0.9084\n",
      "14 Validation accuracy: 0.9088\n",
      "15 Validation accuracy: 0.9064\n",
      "16 Validation accuracy: 0.9066\n",
      "17 Validation accuracy: 0.9066\n",
      "18 Validation accuracy: 0.9066\n",
      "19 Validation accuracy: 0.9052\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternativamente podemos pasar una función de regularización a `tf.layers.dense()`, la cual utilizará para crear operaciones que computarán funciones de perdida de regularización, y esta añade estas operacioenes a la colección de pérdidas de regularización.\n",
    "\n",
    "El código inferior es lo mismo que el anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También pdoemos utilizar `partial()` para esquivar los mismos argumentos una y otra vez, ahora también utilizaremos la función `kernel_regularizer` como argumento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_dense_layer = partial(\n",
    "    tf.layers.dense, activation=tf.nn.relu,\n",
    "    kernel_regularizer=tf.contrib.layers.l1_regularizer(scale))\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    hidden2 = my_dense_layer(hidden1, n_hidden2, name=\"hidden2\")\n",
    "    logits = my_dense_layer(hidden2, n_outputs, activation=None,\n",
    "                            name=\"outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seguido debemos añadir las pérdidas de regularización a la pérdida de base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):                                     # not shown in the book\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(  # not shown\n",
    "        labels=y, logits=logits)                                # not shown\n",
    "    base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")   # not shown\n",
    "    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    loss = tf.add_n([base_loss] + reg_losses, name=\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo demás es como lo usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.8274\n",
      "1 Validation accuracy: 0.8766\n",
      "2 Validation accuracy: 0.8952\n",
      "3 Validation accuracy: 0.9016\n",
      "4 Validation accuracy: 0.9084\n",
      "5 Validation accuracy: 0.9096\n",
      "6 Validation accuracy: 0.9124\n",
      "7 Validation accuracy: 0.9154\n",
      "8 Validation accuracy: 0.9178\n",
      "9 Validation accuracy: 0.919\n",
      "10 Validation accuracy: 0.92\n",
      "11 Validation accuracy: 0.9224\n",
      "12 Validation accuracy: 0.9212\n",
      "13 Validation accuracy: 0.9228\n",
      "14 Validation accuracy: 0.9224\n",
      "15 Validation accuracy: 0.9216\n",
      "16 Validation accuracy: 0.9218\n",
      "17 Validation accuracy: 0.9228\n",
      "18 Validation accuracy: 0.9216\n",
      "19 Validation accuracy: 0.9214\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta técnica es comun en redes neuronals, propuesta por G. Hinton en el 2012 y detallada en un paper de Nitish Srivastva ha sido probada dar buenos resultados (generalmente altos).  Añadir dropout puede aumentar 1-2% de precisión.\n",
    "\n",
    "El algoritmo es simple, realiza una discriminación de neuronas basadas en su probabilida de salida, aquellas que no cumplan serán 'apagadas' o sea ignoradas del entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-114-7adcf6dce8db>:4: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-114-7adcf6dce8db>:4: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n"
     ]
    }
   ],
   "source": [
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "dropout_rate = 0.5  # == 1 - keep_prob\n",
    "X_drop = tf.layers.dropout(X, dropout_rate, training=training)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=tf.nn.relu,\n",
    "                              name=\"hidden1\")\n",
    "    hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training=training)\n",
    "    hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, activation=tf.nn.relu,\n",
    "                              name=\"hidden2\")\n",
    "    hidden2_drop = tf.layers.dropout(hidden2, dropout_rate, training=training)\n",
    "    logits = tf.layers.dense(hidden2_drop, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.9246\n",
      "1 Validation accuracy: 0.9454\n",
      "2 Validation accuracy: 0.9474\n",
      "3 Validation accuracy: 0.9524\n",
      "4 Validation accuracy: 0.9602\n",
      "5 Validation accuracy: 0.9644\n",
      "6 Validation accuracy: 0.961\n",
      "7 Validation accuracy: 0.9666\n",
      "8 Validation accuracy: 0.966\n",
      "9 Validation accuracy: 0.9678\n",
      "10 Validation accuracy: 0.9694\n",
      "11 Validation accuracy: 0.9674\n",
      "12 Validation accuracy: 0.9704\n",
      "13 Validation accuracy: 0.9692\n",
      "14 Validation accuracy: 0.9726\n",
      "15 Validation accuracy: 0.9712\n",
      "16 Validation accuracy: 0.9746\n",
      "17 Validation accuracy: 0.9724\n",
      "18 Validation accuracy: 0.9732\n",
      "19 Validation accuracy: 0.9752\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch, training: True})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta es [otra técnica](https://en.wikipedia.org/wiki/Norm_(mathematics)#Maximum_norm_(special_case_of:_infinity_norm,_uniform_norm,_or_supremum_norm)) de regularización.\n",
    "\n",
    "Para cada neurona, en contraste de los pesos en donde $||w||_2 \\leq r$ se implementa por medio de actualización de ($w = w*r/||w||_2$)\n",
    "\n",
    "Tensorflow no tiene esta implementada pero no es difícil de realizar.  Debajo se muestran los pasos para implementar este tipo de normalización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, controlamos el peso de la primera capa oculta y creemos una operación que calcule los pesos recortados utilizando la función `clip_by_norm ()`. Luego creamos una operación de asignación para asignar los pesos recortados a la variable de pesos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1.0\n",
    "weights = tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")\n",
    "clipped_weights = tf.clip_by_norm(weights, clip_norm=threshold, axes=1)\n",
    "clip_weights = tf.assign(weights, clipped_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo haremos también para la segunda capa oculta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights2 = tf.get_default_graph().get_tensor_by_name(\"hidden2/kernel:0\")\n",
    "clipped_weights2 = tf.clip_by_norm(weights2, clip_norm=threshold, axes=1)\n",
    "clip_weights2 = tf.assign(weights2, clipped_weights2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Añadimos un inicializador y un Saver:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora podemos entrenar al modelo. Es más o menos lo habitual, excepto que justo después de ejecutar `training_op`, ejecutamos las operaciones` clip_weights` y `clip_weights2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.9566\n",
      "1 Validation accuracy: 0.9704\n",
      "2 Validation accuracy: 0.9716\n",
      "3 Validation accuracy: 0.9766\n",
      "4 Validation accuracy: 0.9772\n",
      "5 Validation accuracy: 0.9776\n",
      "6 Validation accuracy: 0.9818\n",
      "7 Validation accuracy: 0.9806\n",
      "8 Validation accuracy: 0.9798\n",
      "9 Validation accuracy: 0.982\n",
      "10 Validation accuracy: 0.9824\n",
      "11 Validation accuracy: 0.9846\n",
      "12 Validation accuracy: 0.9824\n",
      "13 Validation accuracy: 0.9838\n",
      "14 Validation accuracy: 0.9844\n",
      "15 Validation accuracy: 0.985\n",
      "16 Validation accuracy: 0.9838\n",
      "17 Validation accuracy: 0.9844\n",
      "18 Validation accuracy: 0.9842\n",
      "19 Validation accuracy: 0.9846\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:                                              # not shown in the book\n",
    "    init.run()                                                          # not shown\n",
    "    for epoch in range(n_epochs):                                       # not shown\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size): # not shown\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            clip_weights.eval()\n",
    "            clip_weights2.eval()                                        # not shown\n",
    "        acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid})   # not shown\n",
    "        print(epoch, \"Validation accuracy:\", acc_valid)                 # not shown\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")               # not shown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La implementación anterior es sencilla y funciona bien, pero es un poco desordenada. Un mejor enfoque es definir una función `max_norm_regularizer ()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_norm_regularizer(threshold, axes=1, name=\"max_norm\",\n",
    "                         collection=\"max_norm\"):\n",
    "    def max_norm(weights):\n",
    "        clipped = tf.clip_by_norm(weights, clip_norm=threshold, axes=axes)\n",
    "        clip_weights = tf.assign(weights, clipped, name=name)\n",
    "        tf.add_to_collection(collection, clip_weights)\n",
    "        return None # there is no regularization loss term\n",
    "    return max_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces puede llamar a esta función para obtener un normalizador de norma máxima (con el umbral que desee). Cuando crea una capa oculta, puede pasar este regularizador al argumento `kernel_regularizer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_norm_reg = max_norm_regularizer(threshold=1.0)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                              kernel_regularizer=max_norm_reg, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n",
    "                              kernel_regularizer=max_norm_reg, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El entrenamiento es el habitual, excepto que debe ejecutar las operaciones de recorte de pesos después de cada operación de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.9558\n",
      "1 Validation accuracy: 0.97\n",
      "2 Validation accuracy: 0.9732\n",
      "3 Validation accuracy: 0.9758\n",
      "4 Validation accuracy: 0.9764\n",
      "5 Validation accuracy: 0.979\n",
      "6 Validation accuracy: 0.98\n",
      "7 Validation accuracy: 0.9812\n",
      "8 Validation accuracy: 0.9808\n",
      "9 Validation accuracy: 0.981\n",
      "10 Validation accuracy: 0.982\n",
      "11 Validation accuracy: 0.9816\n",
      "12 Validation accuracy: 0.982\n",
      "13 Validation accuracy: 0.9824\n",
      "14 Validation accuracy: 0.9826\n",
      "15 Validation accuracy: 0.9822\n",
      "16 Validation accuracy: 0.982\n",
      "17 Validation accuracy: 0.9824\n",
      "18 Validation accuracy: 0.9822\n",
      "19 Validation accuracy: 0.982\n"
     ]
    }
   ],
   "source": [
    "clip_all_weights = tf.get_collection(\"max_norm\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            sess.run(clip_all_weights)\n",
    "        acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid}) # not shown\n",
    "        print(epoch, \"Validation accuracy:\", acc_valid)               # not shown\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")             # not shown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Laboratorio\n",
    "\n",
    "Parte 1 - Deep Learning - 50%\n",
    "- Construir una red neuronal con 5 capas ocultas de 100 neuronas cada 1 - 10%\n",
    "- He y ELU como funciones inicialización y de activación - 5%\n",
    "- Use un optimizador basado en adam y early stop con MNIST -5%\n",
    "- Ajuste los hiperparámetros usando cross validation y trate de llegar a una precisión alt - 10%\n",
    "- Utilice ahora batch normalization y deduzca cual fue mejor y porqué - 10%\n",
    "- Añada dropout para prevenir overfitting - 10%\n",
    "\n",
    "Parte 2 - Transfer Learning - 50%\n",
    "- Crear una red neuronal que reuse el modelo de las capas ocultas anteriores (congele todas menos las 2 ultimas capas), reemplace la función de activación por softmax - 25%\n",
    "- Haga transfer learning sobre el modelo guardado anteriormente para entrenar solo los números 5 al 9 - 25%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "nav_menu": {
   "height": "360px",
   "width": "416px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
